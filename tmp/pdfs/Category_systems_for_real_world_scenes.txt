Journal of Vision (2021) 21(2):8, 1–31                                                                                                                              1




Category systems for real-world scenes
                                                                     Centre for Vision and Cognition, Psychology, University
Matt D. Anderson                                                                         of Southampton, Southampton, UK

                                                                     Centre for Vision and Cognition, Psychology, University
Erich W. Graf                                                                            of Southampton, Southampton, UK

                                                                     Centre for Vision Research, Department of Psychology,
                                                                       Department of Electrical Engineering and Computer
James H. Elder                                                           Science, York University, Toronto, Ontario, Canada

                                                                           School of Computing and Information Systems, The
Krista A. Ehinger                                                              University of Melbourne, Melbourne, Australia

                                                                     Centre for Vision and Cognition, Psychology, University
Wendy J. Adams                                                                           of Southampton, Southampton, UK

Categorization performance is a popular metric of scene                                     novel categorization method can be applied to a wide
recognition and understanding in behavioral and                                             range of datasets to derive optimal categorical groupings
computational research. However, categorical constructs                                     and labels from psychophysical judgements of stimulus
and their labels can be somewhat arbitrary. Derived                                         similarity.
from exhaustive vocabularies of place names (e.g., Deng
et al., 2009), or the judgements of small groups of
researchers (e.g., Fei-Fei, Iyer, Koch, & Perona, 2007),
these categories may not correspond with                                                      Category systems for real-world
human-preferred taxonomies. Here, we propose                                                  scenes
clustering by increasing the rand index via coordinate
ascent (CIRCA): an unsupervised, data-driven clustering                                        The visual properties of real-world environments
method for deriving ground-truth scene categories. In
                                                                                            have enormous heterogeneity: no two scenes are
Experiment 1, human participants organized 80
stereoscopic images of outdoor scenes from the                                              exactly alike. Scene categories allow us to organize
Southampton-York Natural Scenes (SYNS) dataset                                              environments into meaningful, discrete classes that
(Adams et al., 2016) into discrete categories. In separate                                  represent their statistical regularities, and provide
tasks, images were grouped according to i) semantic                                         a coarse, efficient description of the environment.
content, ii) three-dimensional spatial structure, or iii)                                   Category membership provides information about the
two-dimensional image appearance. Participants                                              probable activities, objects, and layouts associated with
provided text labels for each group. Using the CIRCA                                        a scene, and serves as a convenient descriptor—most
method, we determined the most representative                                               people can easily visualize the typical characteristics of
category structure and then derived category labels for                                     forests or beaches, for example. It is unsurprising, then,
each task/dimension. In Experiment 2, we found that                                         that categorization performance is a popular metric of
these categories generalized well to a larger set of SYNS                                   scene understanding in behavioral and computational
images, and new observers. In Experiment 3, we tested                                       research.
the relationship between our category systems and the                                          Scene categorization is achieved with impressive
spatial envelope model (Oliva & Torralba, 2001). Finally,                                   efficiency and minimal cognitive resources: novel images
in Experiment 4, we validated CIRCA on a larger,                                            can be categorized from brief presentation durations
independent dataset of same-diﬀerent category                                               (Fei-Fei et al., 2007; Potter, 1976), from only foveal, or
judgements. The derived category systems                                                    only peripheral, visual information (Larson, Freeman,
outperformed the SUN taxonomy (Xiao, Hays, Ehinger,                                         Ringer, & Loschky, 2014; Larson & Loschky, 2009), and
Oliva, & Torralba, 2010) and an alternative clustering                                      in the near-absence of attention (Li, VanRullen, Koch,
method (Greene, 2019). In summary, we believe this                                          & Perona, 2002). The computational processes that
Citation: Anderson, M. D., Graf, E. W., Elder, J. H., Ehinger, K. A., & Adams, W. J. (2021). Category systems for real-world scenes.
Journal of Vision, 21(2):8, 1–31, https://doi.org/10.1167/jov.21.2.8.
https://doi.org/10.1167/jov.21.2.8              Received November 27, 2020; published February 17, 2021                      ISSN 1534-7362 Copyright 2021 The Authors

                                     This work is licensed under a Creative Commons Attribution 4.0 International License.
Journal of Vision (2021) 21(2):8, 1–31                 Anderson et al.                                                      2

underpin this ability have been extensively investigated          superordinate or subordinate distinctions. Similarly, the
(Malcolm, Groen, & Baker, 2016); however, most                    “entry level” (i.e., most quickly accessed level) of object
research does not scrutinize the taxonomical structure            categorization is affected by stimulus typicality and
of applied category systems, that is, the ontological             the observer’s subjective expertise (Johnson & Mervis,
“realness” of the individual categories, the lawfulness           1997; Jolicoeur, Gluck, & Kosslyn, 1984; Murphy &
of the categorical boundaries, the number of categories,          Brownell, 1985; Tanaka & Taylor, 1991).
and so on. In this introduction, we discuss different                The ease of categorization may reflect where
taxonomies of real-world scenes, and review a number              an image sits relative to the boundaries that carve
of visual features that are thought to underpin human             out the “perceptual space” into distinct categories
scene categorization. We discuss the behavioral and               (Sofer, Crouzet, & Serre, 2015). Sampling images
computational evidence that feature diagnosticity                 that maximize the distance to a relevant category
depends on taxonomy, and then outline the importance              boundary (e.g., natural vs. manmade), facilitates
of establishing more rigorous taxonomies.                         category discrimination (Sofer et al., 2015). Thus,
                                                                  the category system, in addition to individual
                                                                  differences, may alter the cues that are informative for
Systems of categorization                                         categorization (Figure 1). In the related case of object
                                                                  categorization, encoding of background/context (Prass,
   Category systems can differ in their descriptive scope:        Grimsen, Konig, & Fahle, 2013), orientation (Hamm
a single environment might be described as “natural,” “           & McMullen, 1998), and high spatial frequencies
forest,” or “deciduous thicket.” Each description carries         (Collin, 2006; Collin & McMullen, 2005) varies over
a different amount of detail. Tree hierarchies have               different category systems (i.e., over different levels
been used to represent the multilevel organization of             in the tree hierarchy of categories). As we explore
categories (Rosch, 1975; Rosch, 1999; Rosch & Lloyd,              in this article, similar effects have been observed for
1978; Rosch & Mervis, 1975; Tversky & Hemenway,                   scene categorization. This interplay between perceptual
1983). Superordinate categories (e.g., natural vs.                coding and categorization highlights the importance
man-made or indoor vs. outdoor distinctions) are                  of understanding the category systems that humans
located at the highest tier of the hierarchy, basic-level         naturally use.
categories describe variations within superordinate
categories (e.g., mountain and coast are subdivisions of
natural scenes) and subordinate categories capture finer          Objects
distinctions within basic-level categories (e.g., pebbly             The hierarchical architecture of the human visual
beaches and sea cliffs within coastal scenes).                    system suggests that complex perceptual representations
   Rosch and Lloyd (1978) argue that “the task of                 are built from collections of simpler components
category systems is to provide maximum information                or features. Early theories similarly held that scene
with the least cognitive effort” (p. 10) and propose that         recognition proceeds from an initial stage in which
basic-level categories offer the most economical mode             the identity and position of individual shapes or
of description. Indeed, basic-level names are usually             objects is determined (Bulthoff & Mallot, 1988;
the default: we tend to describe a scene as a “forest,”           Hildreth, 1987; Marr, 1982; Watt, 1990). Experimental
avoiding coarser descriptions such as “natural,” or               work suggests that object identification improves
finer qualifications like “a coniferous forest in autumn”         subordinate scene category discrimination (Collin &
(Hajibayova, 2013). Basic-level categories purportedly            McMullen, 2005; Malcolm, Nuthmann, & Schyns,
offer an optimal trade-off between distinctiveness and            2014). However, basic-level and superordinate scene
informativeness (Murphy & Smith, 1982; Rosch &                    categories are identified in parallel with object
Lloyd, 1978; Tversky & Hemenway, 1983), and, unlike               categories (Fabre-Thorpe, 2011; Joubert, Rousselet,
superordinate categories, may be encoded automatically            Fize, & Fabre-Thorpe, 2007; Rousselet, Joubert, &
or involuntarily in response to visual images (Greene &           Fabre-Thorpe, 2005; VanRullen & Thorpe, 2001), and
Li, 2014).                                                        computing superordinate or basic-level scene category
                                                                  from object statistics is computationally expensive
                                                                  (Greene, 2013). These results suggest that objects are
Category systems and feature encoding                             more useful for subordinate categorization, possibly
                                                                  owing to stronger object predictability for subordinate
   Although there seems to be a general preference                categories, or redundancy between object identities
for using basic-level categories, factors including               and concurrently available low-level image features for
stimulus presentation duration (Kadar & Ben-Shahar,               coarser category discriminations (discussed elsewhere
2012; Loschky & Larson, 2010), presentation order                 in this article). It has also been argued that objects may
(Mack & Palmeri, 2015), and familiarity (Anaki &                  be more frequent and diverse in indoor scenes (Greene,
Bentin, 2009), can bias scene categorization toward               2013). As a result, empirical measurements of the utility
Journal of Vision (2021) 21(2):8, 1–31                      Anderson et al.                                                         3




Figure 1. A toy example of how particular category structures can determine the visual features informative for discrimination. Color
cues may be suﬃcient to discriminate natural images from street images (A), but less informative for discriminating subsets of natural
scenes (B, ﬁeld and forest categories). In this case, information about the spatial structure (among other features; see below for
discussion) would be useful.



of object identification for scene categorization may                  properties, which in turn predict semantic categories, a
depend on the prevalence of indoor categories in the                   core tenet of the spatial envelope model is that category
dataset or experiment.                                                 membership can be determined without parsing an
                                                                       image into its constituent objects (Oliva & Torralba,
                                                                       2001).1
Spatial layout                                                            The spatial envelope model predicts that category
   According to the spatial envelope model (Oliva &                    systems that maximize between-category differences in
Torralba, 2001, 2006; Torralba & Oliva, 2002, 2003), an                coarsely localized GIST features will be discriminated
image’s semantic category (e.g., beach vs. forest) can be              more efficiently by humans. Although basic-level
recovered using a small set of image descriptors termed                categories are thought to be encoded more or
spatial envelope properties (e.g., openness, naturalness,              less automatically (as discussed elsewhere in this
roughness) that represent the spatial layout of the                    article), some work has shown that superordinate
scene (Figure 2). Classifiers trained to predict semantic              categorization in fact precedes basic-level categorization
categories from human-labelled spatial envelope                        (Fabre-Thorpe, 2011; Kadar & Ben-Shahar, 2012;
properties perform similarly to humans (Greene &                       Loschky & Larson, 2010; Sun, Ren, Zheng, Sun,
Oliva, 2006, 2009b). Moreover, adaptation studies                      & Zheng, 2016). According to the spatial envelope
suggest that human category representations rely on                    model, this superordinate advantage emerges because
spatial envelope properties, or correlated features:                   superordinate categories are more separable in
after prolonged viewing of an image set with similar                   the GIST feature space than basic-level categories
spatial envelope properties, subsequent categorization                 (Loschky & Larson, 2010; Oliva & Torralba, 2001).
is biased away from the adaptation set (Greene &                       Indeed, if a biased sample of images maximizes the
Oliva, 2010). For example, adaptation to images high                   discriminability of basic-level members in GIST-space,
in openness generates a bias toward low-openness                       basic-level categorization precedes superordinate
categories such as forests. Spatial envelope properties                categorization (Sofer et al., 2015). The spatial envelope
may be computed from statistics of low-level visual                    model also predicts that GIST features discriminate
features (e.g., histograms of edges or Fourier amplitude               man-made vs. natural categories better than indoor
spectra) pooled over large areas of the visual field                   vs. outdoor categories (Oliva & Torralba, 2001),
(Oliva & Torralba, 2001). For example, human-rated                     and behavioural work confirms that natural vs.
spatial envelope properties can be predicted by the                    man-made category distinctions are faster than
GIST image descriptor proposed by Oliva and Torralba                   indoor vs. outdoor distinctions (Banno & Saiki,
(2001). The GIST descriptor computes a histogram                       2015; Kadar & Ben-Shahar, 2012). In other words,
of average responses to Gabor-like filters at different                not all superordinate categories are distinguished
orientations and scales over different spatial regions                 equally easily, and this could reflect differences in the
of an image (usually a 4 × 4 grid). Because global                     perceptual availability of discriminative spatial layout
GIST features predict human-rated spatial envelope                     information.
Journal of Vision (2021) 21(2):8, 1–31                       Anderson et al.                                                         4

                                                                        than categorization models based on GIST, or other
                                                                        low-level image statistics (e.g., Oliva & Schyns, 2000;
                                                                        Oliva & Torralba, 2001), the latter models were
                                                                        expressly formulated to reveal the diagnostic features
                                                                        of early visual representations and were tested on
                                                                        superordinate or basic-level categories (e.g., the spatial
                                                                        envelope model was formulated to discriminate eight
                                                                        basic level categories; Oliva & Torralba, 2001). In
                                                                        contrast, Greene et al. (2016) tested these models on
                                                                        311 subordinate categories. Low-level image statistics
                                                                        may be more useful for basic- or superordinate-level
                                                                        scene categorization, whereas subordinate-level
                                                                        scene categorization may be more closely related to
                                                                        affordances.

                                                                        Color
                                                                           The greenness of forests, blueness of coastlines, and
                                                                        yellowness of deserts are highly predictive low-level
                                                                        features for categorization (Goffaux et al., 2005).
                                                                        Abnormally colored scenes (e.g., a beach scene with a
Figure 2. The spatial layout of natural scenes correlates with          yellow sky and blue sand) that contain the same color
low-level global image features. (A) Images from the SYNS               segmentation cues as normal scenes (i.e., with similar
dataset (Adams et al., 2016). (B) Variations in pixel intensity         discontinuities in hue at object/surface boundaries) take
across the image capture some characteristics of the scene’s            longer to categorize (Castelhano & Henderson, 2008;
spatial structure without object segmentation. Height                   Goffaux et al., 2005; Oliva & Schyns, 2000). Hence,
corresponds to pixel luminance (images were low-pass ﬁltered            color improves categorization not only because it may
with a gaussian kernel: bandwidth = 50 pixels). (C)                     benefit segmentation, but because some categories have
Spectrograms provide a visualization of the distribution of             well-defined color profiles. Color-based improvements
low-level image features. High energy is indicated in each polar        are larger for indoor vs. outdoor urban discriminations
plot by dark regions. For example, in the image on the right,           than natural vs. manmade discriminations (Rousselet
contrast energy is concentrated in the lower half of the image,         et al., 2005), presumably because artificially illuminated
where there are horizontal image structures of high spatial             indoor scenes tend to be more “yellowish/brownish”
frequencies (note that the high-energy, dark regions in the             than either natural or manmade outdoor scenes
lower Fourier plots are vertically oriented and close to the            (Rousselet et al., 2005). Computational work confirms
center of each plot). The GIST feature descriptor provides a            that color cues reliably discriminate indoor vs. outdoor
summary of these image spectra. (D) Spatial envelope                    images (Szummer & Picard, 1998; Tong, Shi, Yan, &
properties, such as roughness and openness, can be predicted            Wei, 2017). Clearly, the benefit of color information
from GIST. Figure adapted from Oliva and Torralba (2001).               varies with the distribution of colors within and
                                                                        between category representations.
                                                                           In this brief review of four feature dimensions
Aﬀordances                                                              (objects, spatial structure, affordances, and color),
   An alternative, affordance-centered account                          we have highlighted how the cues informative for
of category representations emphasizes that the                         categorization depend on the taxonomical structure
“conceptual structure of environments is driven                         of the chosen category system. This dependence
primarily by the scene’s functions, or the actions that                 highlights the importance of understanding the actual
one could perform in the scene” (Greene, Baldassano,                    scene taxonomies that humans rely on when viewing
Esteva, Beck, & Li, 2016; Groen et al., 2018). Greene                   real-world scenes. Next, we discuss the strengths and
et al. (2016) suggest that scene categories are better                  weaknesses of existing approaches to taxonomizing
predicted by functional information than features such                  real-world scenes.
as color, spatial layout, attributes (surfaces, materials,
etc.), object co-occurrence statistics, and so on.
Importantly, however, affordances necessitate objects                   Existing scene taxonomies
to be acted upon, or spaces to be acted within—and
thus rely on extracting objects and spatial structure.                    Large-scale databases such as ImageNet (Deng et al.,
Moreover, though Greene et al.’s (2016) affordances                     2009), Places (Zhou, Lapedriza, Xiao, Torralba, &
are stronger predictors of human categorization                         Oliva, 2014), and SUN (Xiao et al., 2010) have used
Journal of Vision (2021) 21(2):8, 1–31                 Anderson et al.                                                     5

WordNet (Miller, 1995) to identify quasi-exhaustive               appearance. In Experiment 2, we label a larger number
dictionaries of category terms. These terms are then              of images from the Southampton-York Natural Scenes
entered into search engines to collect image stimuli.             (SYNS) dataset using these categories, and examine
However, the fine granularity of WordNet labels is                the generalizability of the categories derived from
atypical of human language: humans show up to                     Experiment 1. We also explore the relationships between
32.7% disagreement regarding the meaning of these                 generated categories across the three dimensions. In
labels (Chklovski & Mihalcea, 2003). WordNet was                  Experiment 3, we examine the relationship between
built by expert lexicographers and many terms require             our category systems and the spatial envelope model.
substantial esoteric knowledge. For example, “dolmen,”            Finally, in Experiment 4, we evaluate our method on
“medina,” “indoor cloister,” “mastaba,” and “oast                 a larger and completely independent dataset, using a
house” are all categories from WordNet used in the                different experimental paradigm.
SUN and Places databases (Xiao et al., 2010; Zhou
et al., 2014). Computational work has shown that
merging these fine-grained representations into larger             Experiment 1
clusters improves word-sense disambiguation (i.e., the
identification of the correct meaning of a word, given            Methods
multiple meanings—e.g., “bass”; Navigli, 2006; Snow,
Prakash, Jurafsky, & Ng, 2007), and recent behavioral             Participants
work suggests that humans integrate these senses into
simpler taxonomies with fewer categories (Greene,                    A convenience sample of 24 naïve undergraduate
2019).                                                            and postgraduate students, 19 female, age range:
   Additional problems may stem from the putative                 18–26 years, from the University of Southampton
interchangeability of category terms such as “coast,”             participated as volunteers, or in return for course
“beach,” and “seaside.” Well–documented effects of                credits. Each of the three tasks (semantic, 3D spatial
cognitive-linguistic categories on early visual processing        structure, 2D appearance) was completed by 20
suggest that different category labels may elicit different       participants (individual participants completed two or
visual representations, and different categorization              three tasks each; the order was counterbalanced). For
behavior (e.g., Bentin & Golland, 2002; Goffaux,                  all experiments, informed consent was obtained before
Jemel, Jacques, Rossion, & Schyns, 2003; Schyns                   experimentation, and ethical approval was acquired
& Oliva, 1999). Semantic labels modulate low-level                from the Research Governance Office, University of
visual representations within 44 to 150 ms of stimulus            Southampton.
onset (Boutonnet & Lupyan, 2015; Maier, Glage,
Hohlfeld, & Rahman, 2014; Noorman, Neville, &                     Materials
Simanova, 2018)—a time window in which important                     Eighty full-color stereo-pairs (one randomly sampled
scene properties such as color and spatial structure              pair from every scene) were sampled from the SYNS
are encoded (Cichy, Khosla, Pantazis, & Oliva, 2017;              database (Adams et al., 2016). Stimuli were presented
Goffaux et al., 2005; Ramkumar, Hansen, Pannasch, &               on a dual-monitor display (two 32-inch, 2560 × 1440,
Loschky, 2016). Thus, it is important to establish the            75-Hz, ASUS PB328Q monitors) via a single-bounce
category labels that participants would most frequently           Wheatstone mirror stereoscope at an effective viewing
or naturally use.                                                 distance of 83.5 cm. Stimuli were presented en masse as
                                                                  monoscopic thumbnails (3.98 × 2.64° of visual angle),
                                                                  but observers selected individual images for enlarged
Main research question                                            stereoscopic viewing. (Grouping was performed using
                                                                  stereoscopic images to capture the role of binocular
   We have argued that the taxonomical structure                  depth cues in scene perception.) The stereoscopic
of category systems used in empirical perceptual                  images were displayed at 31.12 × 22.36° of visual angle.
research can undesirably confound scene perception                Every participant viewed the same images. The entire
and categorization responses. To address this problem,            task was programmed in MATLAB (MathWorks, Inc.,
we aim to develop a method to identify the categories             Natick, MA).
that humans most naturally use to taxonomize visual
environments. In Experiment 1, we present a novel
method to derive ground-truth category systems                    Procedure
from human grouping judgements in a flexible image                   Participants sorted images into discrete categories.
sorting and labelling task that minimizes instruction             This task was completed separately for each of the three
and experimenter bias. We present category systems                dimensions. Task instructions informed participants of
for three dimensions: semantics, three-dimensional                the grouping system they would use (see Appendix for
(3D) spatial structure, and two-dimensional (2D)                  full instructions).
Journal of Vision (2021) 21(2):8, 1–31                       Anderson et al.                                                          6




Figure 3. Schematic of the categorization task. Images were initially presented in ﬁve stacks of images (randomly assigned) on the left
margin of the display. Sort: Participants dragged and dropped each image into the workspace, stacking same category images.
Enlarged, stereoscopic versions of each image could be viewed at any time. Group: Participants checked the validity of all categories
and returned to the Sorting stage if any categories contained only one image or if the number of categories fell outside the range of
three to 10. Label: Participants labelled each category.


Semantic task: Images were grouped by the “type of                      scenes (Adams et al., 2016). Categories could contain a
place” (e.g., mountain).                                                minimum of two images.
Three-dimensional spatial structure task: Images                           Each task contained three activities: “Sort,”
were sorted according to their depth structure.                         “Group,” and “Label” (Figure 3). Participants accessed
Participants were encouraged “to think about the                        each activity by clicking corresponding tabs at the
3D model that you would have to physically build                        bottom of the display using the mouse.
to represent each scene” and to consider how the                        Sort: All 80 images were initially stacked in random
physical structure of some scenes might be similar or                   order on the left margin of the display. Participants
different.                                                              created categories by dragging images, one at a time,
Two-dimensional appearance task: Images were grouped                    into the workspace; categories were defined as any set
by their 2D appearance (ignoring variations in 3D                       of overlapping images. Enlarged stereoscopic versions
structure). Participants were instructed to attend to the               of the images were viewed by simultaneously pressing
“colors, patterns . . . or textures,” materials, luminosity,            two mouse buttons.
etc., (e.g., blue or red).                                              Group: Category validity was automatically checked. If
   In every task, participants were urged to consider                   participants generated fewer than three or more than 10
each image in its entirety, and discouraged from                        groups, a thick black frame highlighted all categories. If
focusing on smaller subregions like individual objects.                 any group contained fewer than two images, the invalid
Participants were limited to between three and 10                       category or categories were highlighted. Valid groups
categories. This constraint served as a liberal middle-                 were each highlighted with a differently colored border.
ground between accepted set sizes of superordinate                      Participants continued to the “Label” stage once all
(two to three; e.g., Fei-Fei et al., 2007; Oliva & Torralba,            categories were valid.
2001), and basic-level categories (seven to 13; e.g.,                   Label: Participants typed between one and five labels
Fei-Fei & Perona, 2005; Vailaya, Jain, & Zhang, 1998).                  to describe each group of images.
Although there are undoubtably a larger number of                          Participants had unlimited time to complete each
possible categories than our limit of 10, the SYNS                      categorization task and took an average of 30 minutes
dataset only contains a subset of all possible outdoor                  per task.
Journal of Vision (2021) 21(2):8, 1–31                          Anderson et al.                                                     7

The CIRCA method                                                           increases linearly as a function of n, and increases with
   We developed the clustering by increasing the                           k following a power law (see Supplementary Materials,
rand index via coordinate ascent (CIRCA) method to                         Supplementary Figure S1).
organize images into categories based on psychophysical                       To find the globally optimal clustering, our method
judgements of stimulus similarity. In an experiment                        can be implemented multiple times for different
that generates pairwise similarity responses, such as the                  numbers of clusters. To protect against overfitting, we
sorting experiment described elsewhere in this article, it                 cross-validate clusterings on left-out data using the
is possible to represent each participant’s data using an                  adjusted form of the Rand index (ARI), which controls
n x n similarity matrix that codes the pairwise similarity                 for variation in chance-level agreement as a function
between all images by a series of 1s and 0s (1 if a given                  of the number of clusters (Hubert & Arabie, 1985).2 If
pair were placed into the same group, and 0 if they were                   the validation data are a hard clustering, then the ARI
not). Averaging these matrices across participants gives                   is calculated as in Hubert and Arabie (1985), but if
S, a similarity matrix that codes the average association                  the validation data is a soft clustering (e.g., an average
between every image pair. Our ultimate aim is to                           of responses from multiple observers), which has an
identify the set of categories that maximizes in-group                     undefined number of clusters, then the adjustment to
similarities and minimizes out-group similarities in S.                    the Rand index can be calculated by simulating the
   The Rand index quantifies the agreement between                         agreement between the validation set, and a random
two sets of categories by summing i) the number of                         clustering with the same number of clusters as the
pairs that are in the same category in both sets, and ii)                  model. The soft clustering formulation of the ARI is
the number of pairs that are in different categories for                   then:
both sets, and dividing by the total number of pairs                                      RIm − RIr
(Rand, 1971). A score of 1 represents perfect agreement                           ARI =               (2)
between two sets of categories, and 0 represents no                                        1 − RIr
agreement.
   Because the Rand index quantifies the agreement                         where RIm is the rand index from the model, and RIr is
between sets of hard categories (where each datapoint                      the rand index from the random clustering.
belongs to only one category), and S represents                               A comparison against popular alternative clustering
category membership on a continuum, we adapted the                         algorithms (k-medoids and spectral clustering), reveals
Rand index to determine α, the agreement between                           that our method is more robust against response
the similarity matrix S and a proposed clustering                          noise (Supplementary Materials, Supplementary
c = c1 , c2 ,…cn , where ci represents the category to                     Figure S2). Moreover, we show that our method
which image i has been assigned. Let sij be the (i, j)th                   tolerates high levels of interobserver disagreement,
element of S, measuring the similarity of images i and                     and reproduces the exact clustering given an
j. Then, we define the affinity between c and S as:                        internally consistent set of similarity judgements
                                                                           (Supplementary Figure S3). The code for the MATLAB
            1                                                             implementation of this algorithm is available at:
α=                    (ci = c j ) si j + (ci = c j )(1 − si j ) (1)
      n(n − 1)/2 j>i                                                       https://github.com/mattanderson94/CIRCA_Clustering.

   Our goal is to find the clustering c that maximizes
the affinity α. We maximize α by iterative coordinate                      Statistical analyses
ascent from a random initial clustering. On each                              For the semantic, 3D spatial structure, and 2D
iteration, reassignment of a randomly selected image to                    appearance sorting tasks, we identified the category
a randomly selected category is proposed. Proposals                        system that best represented the grouping judgements
that increase α, and therefore improve the agreement                       of all participants. To this end, we i) identified the
between S and c, are accepted. This process is repeated                    optimal number of categories, ii) determined the
until no move increases α (i.e., until a stationary point                  optimal category for each image, and iii) selected names
is reached). Because it is possible for our method to                      for each category from participants’ labels. We describe
converge at local maxima, this entire procedure is                         our method of solving each of these problems in turn.
repeated from a number of different starting positions                     Identifying the optimal number of categories: First,
(initial clusterings).                                                     for a given task, we identified the optimal number of
   Given n stimuli and k clusters, there are kn                            categories using the CIRCA method. We considered
possible clustering solutions, and, on every iteration                     clusterings with between k = 1:20 distinct categories. To
of coordinate ascent, the maximum number of                                avoid overfitting, we used leave-one-out cross validation
proposals before a single reassignment is n(k − 1).                        (LOOCV) over our 20 participants, leaving each
We empirically tested the time complexity of our                           participant out in turn and calculating the averaged
method on simulated datasets of various sizes, and                         80 × 80 similarity matrix from the remaining 19
found that time-to-convergence (i.e., stationarity)                        participants. We applied our method 1,000 times (i.e.,
Journal of Vision (2021) 21(2):8, 1–31                 Anderson et al.                                                            8

from 1,000 different random initial clusterings) to find
the clustering that produced the highest agreement with
the left-out participant (measured using the ARI). The
optimal number of categories was then identified as
the k that produced the maximum ARI between the
optimized clustering and left-out participants’ data,
averaged across all (left-out) participants.
Defining the optimal group-level solution: Having
identified the optimal number of categories, we
determined the optimal group-level clustering using the
CIRCA method on a similarity matrix based on the
data from all 20 participants. (Here, we used 10,000
random initializations.)
Assigning participant-generated labels to each category:
Next, we assigned labels to the optimal group-level
categories. We used the ARI to quantify the agreement
between every group-level category and every raw
participant-generated category (and associated labels)            Figure 4. The optimal number of categories per dimension was
while holding all other participant-generated and                 determined via LOOCV. The y-axis gives the maximum ARI
group-level categories constant. Consider, for example,           averaged over 20 participants (from 1,000 random
a participant that constructed four categories. First, we         initializations of LOOCV) as a function of the number of clusters,
isolate category 1—and partial out the rest—2, 3 and              k (x-axis). Shaded regions represent ±1 participant standard
4—by assigning them all to a common, second category,             error. Vertical dashed lines identify the global maximum for
and then we apply the same treatment to the group-level           each dimension.
categories. The ARI determines how well the selected
participant’s category (and associated label) describes
the selected group-level category. ARIs for categories            Results
with matching labels from different participants
(i.e., multiple uses) were summed. Pluralisms, nouns,                Figure 4 summarizes the results of the LOOCV
adjectives, and verbs with a common stem were treated             analyses used to identify the optimal number of
as the same—for example, one observer might have                  categories. Within each task or dimension, a single
used the label “Farms,” and another observer, “Farm,”             peak in the average ARI can be observed: the optimal
or “Farming.” The “winning” label with the greatest               number of categories for the semantic, 3D spatial
summed ARI was assigned to each category.                         structure, and 2D appearance categories were six,
    To ensure that the final labels represented all images        four, and five, respectively (vertical dashed lines).
in the category, a secondary label was assigned to a              Interobserver agreement (as indexed by agreement
category where it i) conferred novel meaning beyond               between the derived group-level categories and each
the primary label, and ii) was strongly associated                observer’s data) was substantially higher in the semantic
with the images within the category. To quantify                  task (ARI = 0.59) than the 3D spatial structure (ARI =
requirement (i), we determined the semantic similarity            0.35) and 2D appearance tasks (ARI = 0.37).
between the primary label (i.e., the label with the
greatest ARI per category), and every other label using
spaCy v2.0 (https://demos.explosion.ai/similarity/).              Semantic categorization
Labels with similarity scores of less than 0.50 were                 The images associated with each group-level derived
deemed sufficiently low to capture a new or different             category are presented in Figure 5, with the optimal
meaning. For example, “Beach” and “Seaside” describe              label(s). The category labels are “Nature,” “Road,”
semantically overlapping concepts (semantic similarity            “Residence,” “Farm,” “Beach,” “Car Park,” and
= 0.71) and thus provide redundant information,                   “Commercial.”
whereas “Car Park” and “Commercial” (semantic
similarity = 0.43) refer to different scene types.
Requirement (ii) was met by normalizing per-label                 Three-dimensional 3D spatial structure categorization
ARIs to range from 0 to 1, and rejecting values of less             The optimal 3D spatial structure categories
than 0.65.                                                        are presented in Figure 6. The category labels are
    Data can be downloaded from: https://doi.org/10.              “Cluttered” or “Pointy,” “Closed Off,” “Flat,” and
5258/SOTON/D1649.                                                 “Tunnel” or “Navigable Routes.”
Journal of Vision (2021) 21(2):8, 1–31                     Anderson et al.                                                         9




Figure 5. Images assigned to the six optimal semantic categories. Above each category we present the category labels, which were
derived by summing the ARIs over the multiple uses across diﬀerent participants, and picking the maximum/maxima.


Two-dimensional appearance categorization                             relationships between the category members across our
   The optimal 2D appearance categories are presented                 category systems for the three dimensions.
in Figure 7. The category labels are: “Dark,” “Bright,”
“Blue,” “Green,” and “Brown.”
                                                                       Experiment 2
Discussion
                                                                      Methods
  Experiment 1 included only 80 images—a
manageable number for our grouping task. In                           Participants
Experiment 2, we asked observers to use the labels                       Thirty-three naïve undergraduate and postgraduate
derived in Experiment 1 to categorize a larger set of                 students, 27 female; age range: 18 to 23 years, from the
images from the SYNS dataset. We then i) test how well                University of Southampton, none of whom participated
the categories developed in Experiment 1 generalize to                in Experiment 1, were recruited as volunteers, or
the new stimuli and new observers, and ii) evaluate the               in return for course credits. Twenty completed the
Journal of Vision (2021) 21(2):8, 1–31                      Anderson et al.                                                           10




Figure 6. Images assigned to the four optimal 3D spatial structure categories. Above each category, we present the category labels,
which were derived by summing the ARIs over the multiple uses across diﬀerent participants, and picking the maximum/maxima.


semantic categorization task, 20 completed the 3D                      stereoscopic images, subtending 31.12 × 22.36° of
spatial structure task, and 20 completed the 2D                        visual angle (the same size as the large-scale images in
appearance task (participants performed one or                         Experiment 1).
two tasks each; the order was counterbalanced).
Informed consent was obtained before experimentation,
and ethical approval was acquired from the
Research Governance Office, University of                              Procedure
Southampton.                                                              Separately for semantics, 3D spatial structure,
                                                                       and 2D appearance, participants classified every
                                                                       image according to the category labels derived in
Materials                                                              Experiment 1. Participants viewed one image at a time
   For each of the 80 outdoor scenes in the SYNS                       and used a mouse to select the appropriate category
database (Adams et al., 2016), 18 stereo pairs                         label from the list displayed to the side of the image.
compose a 360° panorama of each environment.                           Once a label was chosen, participants continued to
Adjacent stereo pairs overlap, so we selected every                    the next image or trial. Participants categorized all
other image—nine from each scene—to obtain a                           720 images. The image order was randomized between
total of 720 images. Participants viewed full-size                     participants.
Journal of Vision (2021) 21(2):8, 1–31                     Anderson et al.                                                       11




Figure 7. Images assigned to the ﬁve optimal 2D appearance categories. Above each category we present the category labels, which
were derived by summing the ARIs over the multiple uses across diﬀerent participants, and picking the maximum/maxima.



                                                Semantic                     3D spatial structure                    2D appearance

Chance                                           16.67%                             25%                                  20%
80 images from Experiment 1                      82.19%                            70.19%                               71.13%
Remaining 640 images                             82.30%                            71.71%                               68.19%
Table 1. Average interparticipant agreement in Experiment 2 by category systems (columns) and image subset (rows).



Results                                                               separate set of images, we examined interparticipant
                                                                      agreement for the 80 images used in Experiment 1,
  Per-image category membership was determined by                     and the 640 remaining images (Table 1). Category
the most frequently selected category label. To quantify              judgements for the 80 images from Experiment 1
how well the categories derived in Experiment 1                       showed high agreement across the new participants in all
generalized to a separate group of participants, and a                three category systems. As in Experiment 1, agreement
Journal of Vision (2021) 21(2):8, 1–31                       Anderson et al.                                                        12




Figure 8. We examined the strength of the relationship between categories from diﬀerent dimensions by (A) computing the phi
coeﬃcients between diﬀerent categories and (B) testing the performance of a Bayes classiﬁer trained to predict the category of an
image, via leave-one-out cross-validation. In A, the left, middle, and right panels show the association between semantics and spatial
structure, semantics and 2D appearance, and spatial structure and 3D appearance respectively. In B, confusion matrices show the
predictions from three non-naïve Bayes classiﬁers: Spatial structure and 2D Appearance → Semantics (left), Semantics and 2D
Appearance → Spatial Structure (middle), and Spatial structure and Semantic → 2D appearance (right). Rows are model
predictions and columns are the true categories.



was greatest in the semantic task. This result thus                        Next, we determined whether the relationships
shows that our category systems generalize well to new                  between category systems for the three dimensions were
observers.                                                              sufficient to drive reliable classification. In other words,
   Agreement for the new set of 640 images was similar                  we asked whether we can predict an image’s category in
to that for the original 80 images from Experiment 1.                   one dimension from its category in one or both of the
Note, however, that for each new image, there is an                     other dimensions. To explore this hypothesis, we used
image from Experiment 1 taken from the same location,                   Bayes classifiers trained and tested via LOOCV. Table 2
but with a nonoverlapping field of view. This result                    presents the average classification accuracy over 720
thus shows that our category systems generalize well to                 left-out images for every combination of the category
new images, but it remains uncertain how well they will                 systems.
generalize to entirely new locations.                                      Reliable relationships between category membership
                                                                        across the three dimensions are indicated by the
                                                                        fact that all classifiers performed better than chance
                                                                        (1/k), and better than a prior-only model in which
Intercategory relationships                                             the most prevalent or probable category is always
   Phi coefficients (rϕ ) quantify the Pearson correlation              selected. The two-predictor classifiers outperformed the
between images with binary-coded categorical identity                   single-predictor classifiers, with the exception of 3D
(images were either a member or not a member of a                       spatial structure, which was more accurately classified
specified category). Positive values correspond to high                 from semantic structure alone than both semantics and
categorical similarity (images were frequently placed                   2D appearance.
in both categories), and negative values correspond to                     Accuracy alone offers a limited picture of the
low categorical similarity (images frequently placed                    behavior of these models. Consequently, for each
in one category were seldom placed in the other                         of the non-naïve classifiers, we plot confusion
category). Figure 8A illustrates the intercategory                      matrices between the true categories and predicted
correlations for our three category dimensions. Using                   categories (from two predictors). The results are
this metric, intuitive intercategory relationships emerge               shown in Figure 8B. We found that categories vary
(e.g., Nature and Green, Beach and Blue, Residence                      substantially in difficulty. The semantic classifier
and Closed Off, and so on, are all positively correlated).              accurately discriminated most “Nature” images, and
Journal of Vision (2021) 21(2):8, 1–31                                 Anderson et al.                                                     13

                                                                                Predictor dimension(s)

Predicted dimension             Chance        Prior-only      Semantic         Structure     Appearance      Both (naïve)    Both (non-naïve)

Semantic                        16.67%            37.36%          –             53.47%         49.72%          55.97%            57.36%
Structure                       25.00%            31.53%       60.00%              –           44.72%          58.19%            57.92%
Appearance                      20.00%            35.69%       43.75%           42.22%            –            45.83%            50.56%
Table 2. Bayes classiﬁcation accuracy per dimension. Classiﬁers tested using LOOCV on individual images.


Predicted dimension                  Prior-only            Semantic           Structure         Appearance           Naïve         Non-naïve

Semantic                                 37.36%               –                 42.27%            44.65%            48.44%          48.08%
Structure                                31.53%            15.33%                  –              30.58%            26.05%          50.69%
Appearance                               35.69%            39.49%               33.24%               –              39.60%          41.27%
Table 3. Naïve and non-naïve LOOCV (on individual participants) Bayes classiﬁcation accuracy per dimension (i.e., category system).


produced reasonable predictions for “Car Park”                                    predictions were well above chance and prior-only
images, but performed much more poorly on the                                     predictions, and the non-naïve Bayes model performed
other categories. A similar picture emerges for spatial                           better than the naïve Bayes model for two of the three
structure classifier, which discriminated only the                                category systems, and only marginally worse for the
“Closed Off” and “Flat” categories well, and for the                              third (semantic). This shows that the interdependence
2D appearance classifier, which discriminated only the                            between classification systems is relatively stable across
“Green” categories well. These results indicate that                              participants.
the relationships observed between the three category
systems may be limited to a subset of categories; not all
categories are equally predictable.
                                                                                  Typical exemplar classiﬁcation
   We then explored two different ways to combine
the two predictors. Let C j represent the set of possible                            Typical category instances can be defined as images
categories for dimension j and Cij represent the category                         with high interparticipant agreement; atypical images
of image i in this dimension. In a naïve Bayes model,                             can be defined by low interparticipant agreement,
we assume that the two predictors are independent and                             that is, they are associated with multiple categories.
factor the likelihoods. For example, when predicting                              Typical exemplars have a special status in category
the category membership for dimension one from                                    representations: they share many features with
dimensions two and three, we compute:                                             other members of the same category, and few with
                                                                                  members of other categories (Rosch & Mervis, 1975).
       Ci1 = arg max p (Ci1 = c|Ci2 ) p (Ci1 = c|Ci3 )                (3)         Global image features—including color and spatial
                  c∈ CC1                                                          structure—are more predictive of typical category
                                                                                  exemplars than atypical category members (Ehinger,
  In our non-naïve Bayes model, we do not assume                                  Xiao, Torralba, & Oliva, 2011; Torralbo et al., 2013).
independence, and use the joint distribution:                                        We examined classification accuracy as a function
                                                                                  of typicality by selecting the 30 images from each
       Ci1 = arg max p (Ci1 = c|Ci2 , Ci3 )            (4)                        category with the highest interparticipant agreement.
                  c∈ CC1                                                          This produces a uniform prior, such that chance and
                                                                                  “prior-only” performance is equated across categories
   The non-naïve Bayes model performed better than                                as 1/k. Classification accuracy for these typical images
the naïve Bayes model when predicting category                                    was compared against accuracy for two other images
membership within two of the three dimensions,                                    subsets: one consisting of 30 atypical images, that is,
and only marginally worse for the third (3D spatial                               those with the lowest interparticipant agreement per
structure). This reveals a non-trivial interdependence                            category, and one consisting of 30 randomly selected
between the classification systems for each dimension.                            images per category. Once again, we used LOOCV to
   To assess the consistency of this interdependence                              train and test each classifier.
across participants we tested naïve and non-naïve                                    The typical exemplar classifier outperformed
classifiers using LOOCV on N – 1 participants,                                    the random and atypical image classifiers in every
evaluating how well each classifier predicted the left-out                        combination of categories (see Figure 9). This
human categorization judgements (see Table 3). Again,                             typicality advantage was particularly large for
Journal of Vision (2021) 21(2):8, 1–31                       Anderson et al.                                                           14




Figure 9. Bayes classiﬁcation accuracy for random, atypical, and typical images. Most classiﬁers exceeded chance (1/k, dashed line),
and typical exemplar classiﬁcation was consistently more accurate than atypical and random image classiﬁcation.




semantic classification using 3D spatial structure                      our 3D spatial structure categories. Spatial envelope
and 2D appearance (left panel), and 3D spatial                          properties (e.g., Roughness and Openness) are thought
structure classification (middle panel) using semantic                  to dominate early scene representations (Greene &
categories. Most of the typical exemplar classifiers also               Oliva, 2009a, 2009b, 2010) and may be encoded in
outperformed the full-dataset classifiers from Table 2,                 a distinct cortical pathway that represents spatial
despite comparatively small training dataset sizes (120–                boundaries (Harel, Kravitz, & Baker, 2012; Park,
180 vs. 720). These results confirm that relationships                  Brady, Greene, & Oliva, 2011). Moreover, prior work
between dimensions are strongest for typical category                   has found that spatial envelope properties predict
exemplars.                                                              semantic categories, but—notably—using a different
                                                                        set of semantic categories than those we derive from
                                                                        SYNS in Experiment 1 (Greene & Oliva, 2006, 2009b;
Discussion                                                              Oliva & Torralba, 2001). Here we ask whether spatial
                                                                        envelope properties predict our SYNS-derived 3D
   In Experiment 2, we demonstrated that the categories                 spatial structure and semantic categories.
developed from 80 images in Experiment 1 generalized                       The relationship between low-level features that
well to 640 additional images, and different participants.              comprise an image’s GIST (see Figure 2), and spatial
Our category systems were not only representative                       envelope properties, may vary across datasets. Hence,
of the 80 images they were derived from, they also                      we also test whether GIST features are consistently
captured the categorical structure of new images.                       diagnostic of spatial envelope properties (regardless of
However, given that generalization was only tested for                  the dataset), or whether this relationship is unstable and
images sampled from the same dataset, it is still possible              idiosyncratic. Previous computational work suggests
that each category system reflects idiosyncrasies of                    that cluster-weighted models (CWMs) applied to GIST
the images used to develop them (in our case, the                       features are “well suited to encoding structural scene
SYNS dataset). The SYNS scenes were randomly                            priors” (Ross & Oliva, 2010, p. 21), so we examined
sampled from a diverse range of outdoor environments                    whether we could apply CWMs to the GIST features
identified in the UKLand dataset (GeoInformation                        of SYNS images to predict human spatial envelope
Group, www.geoinformationgroup.co.uk) to capture a                      ratings.
wide variety of real-world scenes (Adams et al., 2016).                    To summarize, in Experiment 3 we ask human
Although we hope that this careful sampling will lead                   observers to directly estimate three spatial envelope
to good generalization, it remains to be seen how the                   properties of SYNS images (mean depth, openness
category systems derived from our first experiment                      and perspective). We examine how well these spatial
generalize to other image datasets.                                     envelope properties can be used to classify the SYNS
   A second question is how the categories we derived                   images across the three category systems developed in
in Experiment 1 relate to existing models of scene                      Experiment 1, and assess improvements in classification
categorization. We address this issue in our third                      as a function of typicality (as in Experiment 2). Finally,
experiment. The spatial envelope model (Oliva &                         we quantify the relationship between SYNS image
Torralba, 2001) serves as a good comparison for                         GIST features and spatial envelope properties, with an
Journal of Vision (2021) 21(2):8, 1–31                  Anderson et al.                                                          15

aim to replicate and generalize the results from Ross                                                      Subset
and Oliva (2010).
                                                                                          All 720    Typical   Atypical    Random

                                                                   Semantic               45.76%     44.44%     37.22%     39.44%
 Experiment 3                                                      3D spatial structure   67.73%     85.83%     64.17%     62.50%
                                                                   2D appearance          39.08%     40.14%     45.77%     45.07%
Methods                                                            Table 4. Naïve Bayes classiﬁcation accuracy per dimension (i.e.,
                                                                   category system). Models were trained to predict each
Participants                                                       dimension from three human-rated spatial envelope
                                                                   dimensions, namely, openness, mean depth, and perspective.
  Three postgraduate students, 2 male (including M.A,
who was the only non-naïve participant), age range:
23–27 years, from the University of Southampton
participated as volunteers. Informed consent was                   envelope properties is modulated by this spatial
obtained before experimentation, and ethical approval              resolution. We therefore determined the optimal spatial
was acquired from the Research Governance Office,                  resolution for (independently) predicting the three
University of Southampton.                                         spatial envelope properties from SYNS image GIST
                                                                   features. First, we projected the GIST features onto
                                                                   the PCA bases derived by Ross and Oliva (computed
Materials                                                          from an independent, third dataset—a measure taken
   Image and display specifications matched those                  to facilitate model generalization). Subsequently, using
reported in Experiment 2.                                          five-fold cross validation, we trained CWMs to predict
                                                                   the human-generated spatial envelope properties from
                                                                   these GIST features, recording mean squared prediction
Procedure                                                          errors over each left-out fold. Because CWMs are
   We replicated the task performed by Ross and Oliva              optimized for estimating data with context-dependent
(2010), wherein participants viewed one monoscopic                 relationships between inputs and outputs (e.g., in
image at a time, and used three sliders to quantify the            our case, an enclosed forest scene may have different
“Mean Distance,” “Openness,” and “Perspective” on                  low-level features to an enclosed street scene; for details,
a scale of 1 to 7.3 Participants rated all 720 images in           see Ross & Oliva, 2010), we also cross-validated, within
random order.                                                      each spatial resolution, the optimal number of model
                                                                   clusters. An additional set of models, trained on Ross
                                                                   and Oliva’s (2010) dataset and tested on the SYNS
Statistical analyses                                               dataset, were developed to test generalization of the
                                                                   relationship between GIST features and spatial envelope
   First, we report human-rated spatial envelope
                                                                   properties (reflected in prediction accuracy relative to
properties across the image categories developed
                                                                   models trained and tested on the same dataset).
in Experiments 1 and 2. Second, we explore the
relationship between image GIST features (see Figure 2)
and human-rated spatial envelope properties.
Specifically, we test whether CWMs operating on image              Results
GIST features provide a good, generalizable model
of human perception of spatial layout, as suggested                Human ratings
by Ross and Oliva (2010). Accordingly, we predicted                   Figure 10 shows the human spatial envelope ratings
spatial envelope properties from image GIST features               and the CWM-estimated ratings separated by 3D spatial
using CWMs: i) trained and tested on SYNS images                   structure category over all 720 SYNS images. Intuitive
and ratings or ii) trained on Ross & Oliva’s images                patterns are evident in the human ratings. For example,
and ratings, and tested on SYNS images and ratings                 the closed off category has low values on all three spatial
(see Table 4). Our procedure, detailed elsewhere in                envelope properties, whereas the flat category is high in
this section, replicates Ross & Oliva’s cross-validation           openness and mean depth, and low in perspective. These
method.                                                            results confirm that our 3D spatial structure categories
   By dividing an image into spatial grids of varying              capture environmental regularities also conveyed by
size (e.g., 2 × 2 or 4 × 4) and computing the GIST                 human-rated spatial envelope properties. Indeed, a
features at every grid location, we can obtain GIST                Naïve Bayes classifier trained to predict 3D spatial
representations with different spatial resolutions.                structure categories from the three human-rated spatial
Ross and Oliva (2010) found that the strength of                   envelope properties (via LOOCV, as in Experiment 2),
the relationship between GIST features and spatial                 achieved 67.73% accuracy (Table 4)—substantially
Journal of Vision (2021) 21(2):8, 1–31                   Anderson et al.                                                    16

                                                                    spatial envelope properties to predict 3D spatial
                                                                    structure, semantic, and 2D appearance categories, we
                                                                    achieved 85.83%, 44.44%, and 40.14% classification
                                                                    accuracy, respectively (see Table 4). Although 3D
                                                                    spatial structure classification showed a substantial
                                                                    improvement, semantic and 2D appearance categories
                                                                    produced negligible changes. This pattern was also
                                                                    found for atypical and randomly sampled images: 3D
                                                                    spatial structure classification was substantially poorer
                                                                    for atypical and random exemplars, but semantic and
                                                                    2D appearance classification was relatively unaffected
                                                                    (Table 4).


                                                                    CWM performance
                                                                       Table 5 shows the CWM prediction errors and
                                                                    optimal CWM parameters for predicting human-rated
                                                                    spatial envelope properties from GIST, within and
                                                                    across datasets. CWMs learn optimal regression
                                                                    functions to apply for specific contexts, thereby
                                                                    obtaining more accurate predictions than standard
                                                                    linear models (Ross & Oliva, 2010). Ross and Oliva’s
                                                                    (2010) images required higher spatial resolutions than
                                                                    SYNS images to optimally estimate spatial envelope
                                                                    properties (see Table 5). Moreover, training and testing
                                                                    across different datasets caused a substantial increase in
                                                                    prediction error (compare the SYNS/SYNS and Oliva &
                                                                    Ross/SYNS mean squared prediction errors in Table 5).
                                                                    These results suggest that the relationship between
                                                                    GIST features and spatial envelope properties varies
                                                                    between datasets. This finding cannot be attributed to
                                                                    weaker relationships between GIST features and spatial
                                                                    envelope properties, or poor suitability of CWMs
Figure 10. Mean human ratings (‘Human’) and CWM-estimates           for the SYNS images, because CWMs trained and
(‘SYNS-Est’ and ‘Oliva&Ross-Est’) for three spatial envelope        tested on SYNS produce smaller errors across all three
properties separated by 3D spatial structure category. Error        dimensions.
bars show ±1 standard deviation.                                       Inspection of the model-estimated spatial envelope
                                                                    properties across our 3D spatial structure categories
                                                                    in Figure 10 illustrates that the CWMs trained on SYNS
better than “prior-only” classification, predictions from           data generated predictions that, for the most part,
semantics, 2D appearance, or both (Table 2).                        preserved category-specific patterns of spatial envelope
   Semantic and 2D appearance categories were                       ratings: the closed off category produced relatively low
classified from human spatial envelope ratings with                 values across all three dimensions, and the flat category
45.76% and 39.08% accuracy, respectively (see Table 4).             was low in perspective, but high in openness and mean
It is worth noting that classifiers using only an image’s           depth (i.e., like the human ratings). By contrast, the
3D spatial structure category to predict its semantic               CWMs trained on a different dataset markedly distorted
and 2D appearance category performed better than                    these patterns: spatial envelope properties vary little
predictions from these spatial envelope properties                  between categories, and across every category, mean
(Table 2).                                                          depth is substantially overestimated, and openness and
                                                                    perspective are underestimated. To explore this further,
                                                                    we trained two naïve Bayes classifiers to predict our
Typical exemplars only                                              3D spatial structure categories from spatial envelope
   Experiment 2 demonstrated that typical category                  ratings i) estimated from the SYNS-trained model,
exemplars have more predictable features. Isolating                 and ii) estimated from the Oliva and Ross-trained
the 30 images from each category with the highest                   model. Classification was considerably more accurate
interobserver agreement, and using the human-rated                  using SYNS-trained estimations (57.16% vs. 44.92%),
Journal of Vision (2021) 21(2):8, 1–31                      Anderson et al.                                                     17

                                            SYNS/SYNS                         Oliva & Ross/Oliva & Ross          Oliva & Ross/SYNS

Training/test data             Resolution      Clusters   MSE          Resolution        Clusters         MSE          MSE

Mean depth                        1×1             8       0.47            4×4               6             0.56         1.81
Openness                          4×4             5       0.35            8×8               6             0.87         1.91
Perspective                       1×1             5       0.99            2×2               4             1.95         1.30
Table 5. Optimal CWM parameters and mean-squared prediction errors (MSEs) for estimating human-rated spatial envelope
properties from GIST. Note: The cross-validation method for identifying the optimal CWM parameters (spatial resolution and number
of clusters) for predicting the spatial envelope properties of Oliva & Ross’ images is described in Oliva & Ross (2010). The model
trained on Oliva & Ross’ dataset and tested on SYNS used the optimal model parameters for the training data.



confirming that the relationship between GIST and                      performance (e.g., in the burgeoning field of deep
spatial envelope properties is unstable between datasets.              learning). In Experiment 4 we test i) whether our
                                                                       clustering method can be applied to larger datasets
                                                                       and ii) how well the resultant labels capture human
Discussion                                                             classification judgements, relative to the existing ground
                                                                       truth labels for large datasets.
   Human-rated spatial envelope properties are closely                    Clearly, our sorting task of Experiment 1 would
related to our 3D spatial structure categories. Indeed,                become infeasible for datasets containing thousands
the impressive classification performance found for                    of images. However, our method can be applied
typical 3D spatial structure exemplars (i.e., 85.83%)                  to data from various experimental paradigms that
suggests that spatial envelope properties and our                      produce pairwise similarity judgements. Fortunately,
categorical description of spatial structure encode                    appropriate data already exist from a same–different
similar scene properties. However, the relationship                    experiment conducted by Greene et al. (2016).
between spatial envelope properties and our other
category systems (semantic, 2D appearance) was
weak—weaker in fact than the relationship between
our 3D structure categories and those category                         Method
systems.
   Notably, the GIST features that predict spatial                     Participants, materials, and procedure
envelope properties vary between datasets, thereby                        Here we provide a short summary of the study
impeding generalization. Although low-level differences                conducted by Greene et al. (2016). For a complete
between the datasets may account for this effect,                      description of the study, please refer to the original
the sensitivity of GIST to these low-level properties                  paper.
suggests that GIST features may not provide a robust                      A total of 2,296 participants were recruited from
route to scene understanding.                                          Amazon Mechanical Turk (AmTurk), and stimuli were
                                                                       obtained by pooling 62,468 images from ImageNet
                                                                       (Deng et al., 2009), SUN (Xiao et al., 2010), Corel, and
                                                                       an additional 15-scene database (Fei-Fei & Perona,
 Experiment 4                                                          2005; Lazebnik, Schmid, & Ponce, 2006; Oliva &
                                                                       Torralba, 2001).
   In a final experiment, we examine the flexibility of                   For each trial, participants viewed two images side
the CIRCA method by applying it to data collected                      by side, and were asked to determine whether they
from a larger image set, using a different experimental                belonged to the same or different category (via button
task. Databases like ImageNet (Deng et al., 2009), SUN                 press). Categories were defined by the instructions to
(Xiao et al., 2010), and Places (Zhou et al., 2014) use                participants: “Consider the two pictures below, and the
semantic labels to search crowd-sourced photography                    names of the places they depict. Names should describe
sites (e.g., Google images), enabling large-scale image                the type of place, rather than a specific place and
sampling from a wide range of environments (albeit                     should make sense in finishing the following sentence
at the expense of control over intrinsic and extrinsic                 ‘I am going to the. . . .’ ” Participants also named the
camera properties). These large-scale databases are                    category of every left image (as a free-text response).
popular in computer vision and behavioral research,                    Image pairs were selected randomly, and participants
and the categories that organize these databases are                   were remunerated per trial, completing as many trials
frequently used as class labels to evaluate model/human                as they liked.
Journal of Vision (2021) 21(2):8, 1–31                 Anderson et al.                                                            18

Statistical analyses
  To validate the CIRCA method, we compared it
against two competing models.

 (i) The SUN category system (Xiao et al., 2010).
     The majority (68.14%) of Greene et al.’s (2016)
     pooled dataset contains images taken from the SUN
     database. The SUN database was constructed by
     finding 2,500 unique terms in WordNet (Miller,
     1995) that describe real-world environments. After
     collapsing over synonyms and expanding categories
     with multiple visual subtypes (e.g., indoor vs.
     outdoor views of churches), 899 category labels
     emerged, and images for each category were                   Figure 11. The optimal number of categories was determined
     retrieved by downloading the images returned by              via 10-fold cross-validation. The y-axis gives the ARI averaged
     various search engines (e.g., Google Images).                over 10 folds, and 1,000 diﬀerent random initializations of the
(ii) Greene’s (2019) clustering method. Greene (2019)             CIRCA method per fold. Shaded areas around each line
     proposed a simple clustering method using the                represents ±1 standard deviation. The optimal number of
     same–different judgements in the dataset described           categories (vertical dashed lines) is 55 for all 1,000 images, and
     above (Greene et al., 2016). First, images are               24 for Greene’s subset.
     assigned to their respective SUN, ImageNet, and
     Corel categories, and the proportion of trials in
     which observers responded “same” is computed                 vast improvement over 0.27%) had similarity data in
     for images from the same category, and images                this sample.
     from different categories. This process is completed            In Experiment 1, we protected against overfitting
     for every pair of categories to build a by-category          while finding the optimal number of clusters by testing
     similarity matrix. Categories (and the corresponding         model predictions against data produced by left-out
     images) that produce within-category similarities            participants (i.e., leave-one-out cross-validation). As the
     of less than 0.75 are removed. Pairs of categories           current dataset omits participant identifiers, we used
     that produce between-category similarities of greater        k-fold cross-validation on individual trials instead. In
     than 0.5 are merged. Note that this method removes           most other respects, we simply replicated the analyses
     and merges whole categories, and does not operate            described in Experiment 1. In short, we determined the
     on individual images.                                        optimal number of clusters by splitting the trial-by-trial
                                                                  data (i.e., individual similarity judgements) into 10
   For a fair comparison with both of these models,               equally sized folds, training on nine folds, and testing
we only retained the 42,927 images retrieved from                 on each left-out fold in turn.
the SUN database, used by Greene et al. (2016).
Of the approximately 921 million possible pairwise
combinations of these 42,927 images, approximately 2.5            Results
million (0.27%) were presented to participants at least
once. Because the vast majority of image pairs never                 Figure 11 (purple line) shows the resulting ARI using
occurred in this experiment, the resulting 42,927 ×               our 10-fold cross validation over the 1,000 selected
42,927 similarity matrix is highly sparse. Missing data           images, as a function of the number of clusters. The
introduces uncertainty: two images without similarity             curve has been smoothed by kernel regression, with
data could belong to the same or different categories.            kernel scale optimized by leave-one-out cross-validation
To minimize sparsity, we used an iterative sampling               on the mean ARIs. We find that the ARI peaks at 55
procedure to find the most densely connected subset of            clusters, somewhat less than the 72 SUN categories
images (i.e., with the largest number of observations).           present in our sample of 1,000 images.
We first selected the single image with the largest                  After fixing the number of clusters to the optimal
number of unique pairings with other images. Further              number, the CIRCA method was rerun with all of
images were iteratively added to our sample, by finding,          the data included from 10,000 random initializations,
on each iteration, the image with the maximum number              to find the clustering that maximized the ARI. The
of connections (i.e., same–different judgements) with             resulting ARI is slightly higher than the ARI produced
the images already in the sample. For the current study,          by the SUN category system (see Table 6, rows 1–2).
we selected the 1,000 maximally connected images. Of              When we (suboptimally) increased the number of
the 499,500 possible unique pairings, 31,884 (6.38%—a             categories to 72, to match the number of represented
Journal of Vision (2021) 21(2):8, 1–31                          Anderson et al.                                                   19




Figure 12. Cluster similarity between models quantiﬁed using the ARI. (A) In the 1,000-image subset, our model produced clusterings
that were highly similar to the SUN model. (B) In the 712-image subset, we observed similarly high agreement with the SUN
clusterings, particularly when we matched the number of clusters. Greene’s method produces markedly diﬀerent clusterings.


Model           No. of images            No. of categories    ARI          and regardless of whether we used the optimal number
                                                                           of clusters, or simply matched the number of clusters.
CIRCA                 1,000                     55           0.7703           We can also examine the similarity of the clusterings
SUN                   1,000                     72           0.7354        produced by the different methods using the ARI. Our
CIRCA                 1,000                     72           0.7697        method produced clusterings highly similar to the SUN
CIRCA                  712                      24           0.8331        model, whereas Greene’s method produced clusterings
Greene                 712                      22           0.8196        that differed from the other two (see Figure 12). When
CIRCA                  712                      22           0.8328        the number of clusters was matched, there was very
SUN                    712                      35           0.8326        close agreement between our method, derived using
CIRCA                  712                      35           0.8338        human same-different judgements, and the SUN
Table 6. The ARI evaluates how well each model predicts                    system, derived entirely independently, via label-driven
similarity judgements in the same–diﬀerent task. For the                   image searches (ARI = 0.96, Figure 12B).
sample of 712 and 1,000 images, the CIRCA method                              Examples of agreements and disagreement between
outperforms the two alternative models: the SUN category                   the three models are illustrated in Figure 13. While the
system, and Greene’s (2019) clustering method.                             SUN system separates ‘Grotto’ and ‘Underwater Ice’
                                                                           images, our category system combines both into a “Sea”
                                                                           category (our labelling method is described elsewhere in
                                                                           this article). Also, our method splits “Flight of Stairs,
SUN categories, the ARI decreased as expected, but                         Natural” into “Mountain” and “Forest” categories
remained favorable compared with the SUN system                            based on the global context in which the stairs occur.
(Table 6, row 3).                                                          Greene’s method subsumes “Underwater Ice” and
   Greene’s (2019) method removes 26.17% of trials by                      “Underwater Pool” under a single “Sea” category.
excluding categories with an average within-category                       Importantly, however, many categories are identical
similarity rating of less than 0.75, leaving 712 images                    across all three models (green bounding boxes).
organized into 22 categories (merging 35 SUN                                  Another way of comparing our clustering method
categories). For a fair comparison against Greene’s                        to existing models is to analyse/analyze the category
method, we repeated the CIRCA method with this                             labels that observers assigned to every left image in
reduced sample of 712 images. Cross-validation revealed                    Greene’s experiment. A good category system should
that 24 clusters was optimal for this subset (Figure 11,                   maximize the variance in word meaning between
green line) and the resulting optimal clustering ARI                       category labels, and should minimize variance within
compares favorably with Greene’s categories and                            categories. Put simply, categories should represent
the SUN categories (Table 6, rows 4–6). Finally, we                        independent concepts, but members of the same
compared these alternative models to our method for                        category should be relatively homogenous. We quantify
this image subset when we matched the number of                            word meaning using Word2Vec (Mikolov, Sutskever,
clusters (Table 6, rows 7–8). Our method outperformed                      Chen, Corrado, & Dean, 2013). Word2Vec is a family of
the two competing models, regardless of sample size,                       shallow, two-layer neural networks that produce word
Journal of Vision (2021) 21(2):8, 1–31                     Anderson et al.                                                      20




Figure 13. Example categories produced by our clustering method (left column), the SUN system (middle column), and Greene’s
method (right column). To generate these examples, we sampled a small subset of images used in Experiment 3, and assigned them
to their respective categories according to the three diﬀerent category systems. Bounding boxes show the diﬀerent categories, and
labels above each box are the category labels either retrieved from the SUN database (middle column), or derived by computing the
mean word-vector of the participant-generated labels. In the left column, we provide a small number of example labels assigned to
the images by observers. Green bounding boxes (bottom) signify that all three models generated the same category.
Journal of Vision (2021) 21(2):8, 1–31                 Anderson et al.                                                        21

embeddings. These models are trained to predict the               Model         No. of Images       K categories          F
identity of single words from neighboring words taken
from the same sentence (using large-scale corpora).               CIRCA             1,000                55             537.22
The hidden layer varies in dimensionality (from 100 to            SUN               1,000                72             447.36
1,000; Mikolov, Chen, Corrado, & Dean, 2013) and                  CIRCA             1,000                72             417.68
represents a vector space. Each unique word inputted              CIRCA              712                 24            1127.15
during training is assigned a corresponding “word                 Greene             712                 22            1121.06
vector,” or embedding, in this vector space. A useful             CIRCA              712                 22            1190.57
property of these embeddings is that they are organized           SUN                712                 35             815.49
semantically, and can be manipulated algebraically (a             CIRCA              712                 35             794.52
well-established example is king − man + woman =                  Table 7. The F-ratio quantiﬁes the variance in word meaning
queen). For our purposes, Word2Vec offers a useful                captured by the models. Our CIRCA method produced a
quantitative representation of word meaning to evaluate           superior ﬁt to labelling data when we used the optimal number
clusterings.                                                      of clusters (24 and 55), but performed worse when we matched
   We used the GloVe model, which represents each                 the number of clusters in SUN.
label as a 300-dimensional word vector (Pennington,
Socher, & Manning, 2014), to derive word embeddings
for all the participant-generated labels. Word vectors
were converted to unit vectors, and organized into                as castles). This process of simplification is similarly
categories based on the category of the images they               borne out in the clustering results: our method produces
describe. We calculated the grand-mean word-vector,               fewer clusters than the SUN category system.
and, for each category system, the category-mean word-
vectors. Then, we computed i) the (summed) squared
Euclidean distance between the grand-mean and the                 Discussion
category-means (Dbetween ) and ii) the (summed) squared
Euclidean distance between the category-means,                       In Experiment 4, we investigated the scalability
and the individual word-vectors within categories                 of our proposed clustering algorithm. We applied
(Dwithin ). These two distances are the same as to the            our method to human data from a large-scale
between-group and within-group variance estimates in              same–different experiment, and tested our clusterings
the standard F-test. Accordingly, we tested model fit by          against two alternative models: the SUN taxonomy,
calculating the ratio between Dbetween and Dwithin :              and a simple thresholding method proposed by Greene
                                                                  (2019). We found that our method outperformed both
               Dbetween / d f1                                    models. Moreover, we tested whether our category
       F =                               (5)                      system was more consistent with the labels used by
               Dwithin / d f2                                     participants in the same experiment. When we used
                                                                  the optimal number of clusters—determined via
where df 1 and df 2 are the degrees of freedom: df 1 = k −        cross-validation—our clusterings outperformed the
1, and df 2 = N − k, where N represents the number of             SUN category system. These results suggest that our
observations or labels, and k represents the number of            method can be applied to data from different tasks, and
categories.                                                       larger datasets.
    Our clustering method produces higher F-ratios
when we use the optimal (cross-validated) number of
clusters, or match the number of clusterings in Greene’s
method (see Table 7). However, when we match the                   General discussion
number of clusters in SUN, the SUN model achieves a
higher F-ratio.                                                      We proposed a behaviorally grounded method of
    Using the Word2Vec representation, we can derive              deriving category systems for real-world scenes, and
single-word category terms by computing the centroid              validated it on the SYNS (Adams et al., 2016), and
(i.e., mean) word-vector for each category.4 Examples of          SUN databases (Xiao et al., 2010). In Experiment 1, we
category terms produced using this method, alongside              instructed participants to categorize 80 SYNS images
the raw participant-generated labels, are presented               by their i) semantic content, ii) 3D spatial structure,
in Figure 13 (left and right columns, above each                  and iii) 2D appearance, in a free-sorting task. We
box/category). Compared with the category terms used              determined the optimal category structure for each
in the SUN database, these terms are more general. For            task and assigned participant-generated labels to each
example, for the SUN category “Gatehouse,” human                  category.
participants preferred to use the more general term                  In Experiment 2, a separate set of participants used
“Castle” (gatehouses are typically in the same grounds            the optimal labels from Experiment 1 to categorize a
Journal of Vision (2021) 21(2):8, 1–31               Anderson et al.                                                    22

larger set of 720 SYNS images. We produced strong               in past research (e.g., “highway,” “coast,” etc.;
evidence that our category systems generalized over             Fei-Fei et al., 2007; Fei-Fei & Perona, 2005; Oliva &
a larger set of images. Moreover, we found stable               Torralba, 2001). Interestingly, however, most existing
category associations that enabled predictions of               category systems discriminate between forest and
category membership in one dimension from categorical           countryside categories. Forests and countryside are
properties across other dimensions.                             basic-level members of the superordinate nature
   In Experiment 3, we labelled the SYNS dataset                category; existing scene taxonomies assume a sharp
using three spatial envelope properties and found a             division between these two levels of representation,
reliable relationship with the 3D spatial structure             partitioning categories into discrete multilevel
categories, and weaker relationships with the semantic          hierarchies (Rosch & Lloyd, 1978; Tversky &
and 2D appearance categories. We showed that                    Hemenway, 1983). Our “nature” category unifies
without dataset-specific training, GIST features are            forest and countryside scenes, thereby intermingling
not diagnostic of spatial envelope properties or scene          superordinate and basic basic-level categories. This
category.                                                       finding suggests that the accepted demarcation between
   In Experiment 4, we tested our method on data from           superordinate and basic-level scene categories may be
a same-different task using 712 to 1,000 images from            fuzzier than previously thought. Although it is also
the SUN database. Our method generated categories               possible that the “nature” category was produced by
that predict same/different judgements more accurately          averaging over two types of participant, namely, i) those
than the SUN taxonomy, and an alternative clustering            that generated superordinate categories, and ii) those
method (Greene et al., 2016). Moreover, our method              that generated basic-level categories, we introduced
generated categories that captured a greater amount of          a constraint on the number of categories to prevent
variance in the meaning of participant-generated labels.        this problem. Within the specified range of three
                                                                to 10 categories, the minimum number of semantic
                                                                categories used by any participant was five. Hence, it is
Deriving participant-driven category systems                    doubtful that some participants were just performing
                                                                superordinate categorization.
   Image categorization is a popular metric for scene              The variability in the granularity of individual
recognition, yet potential problems with contrived              categories within a category system can be interpreted as
categorical taxonomies of real-world scenes are seldom          an extension of what Rosch and Lloyd (1978) described
discussed. In most categorization research, participants        as the economic balance between low cognitive effort
are presented with category labels that ostensibly              and maximum discriminability (although they asserted
represent the ground-truth categorical structure of             that this was limited to basic-level category systems).
real-world environments. However, different studies             Representing some categories coarsely, and other
use different category systems, under an implicit               categories at a finer level, may be optimal under
assumption that variations in categorical structures            certain conditions. For example, plants and animals
have little or no effect on participant behavior.               are hierarchically classified according to species, genus,
Surprisingly, this assumption is maintained despite the         family, and so on, in Western scientific taxonomies.
known inequality of different categorical descriptions          Many non-Western cultures share similar taxonomies,
(e.g., between basic-level and superordinate category           but eschew some redundant distinctions in favor of
systems; Sofer et al., 2015). We have argued that               more generic categories that have greater cultural
different category systems codify different visual              utility (thereby generating sets of categories with mixed
features and thus experimental categorization tasks will        granularity; for a review, see Malt, 1995). Real-world
produce unnatural behavior insofar as applied category          scene categories may vary with similar observer
systems fail to reflect human-preferred taxonomies              characteristics, such as stimulus familiarity, motivation,
of real-world environments. Our participant-driven              expertise, and of course, culture, that cause humans to
method of deriving category systems directly identifies         use mixtures of coarse and fine distinctions.
these human-preferred taxonomies and thereby                       Our 3D spatial structure categories strongly resemble
provides a means of obtaining a more principled                 Oliva and Torralba’s (2001) spatial envelope properties.
ground-truth.                                                   Categories of “flat” and “closed off” seem to correspond
                                                                to opposing poles along the openness dimension,
                                                                “cluttered” or “pointy” corresponds with “roughness,”
Properties of the SYNS category systems                         and “tunnel” or “navigable routes” resembles the
                                                                “expansion/navigability” dimensions (Greene & Oliva,
   Applying our method to the semantic categorization           2006; Oliva & Torralba, 2001). In Experiment 3, we
task in Experiment 1 generated intuitive labels like            verified this mapping by testing the performance of
“road,” “car park,” “residence,” and “beach”—all                Bayes classifiers trained to predict 3D spatial structure
of which resemble commonly applied categories                   category by encoding variations in human-rated
Journal of Vision (2021) 21(2):8, 1–31                 Anderson et al.                                                    23

spatial envelope properties. We found that spatial                global luminance (bright and dark). Although color
envelope properties were strong predictors of category            is known to be informative for scene understanding
membership, achieving 85.83% classification accuracy              (Castelhano & Henderson, 2008; Goffaux et al., 2005;
for typical category exemplars. The convergence of our            Goffaux et al., 2003; Oliva & Schyns, 2000), no prior
3D spatial structure categorical model and the spatial            studies have investigated the importance of global
envelope model (Oliva & Torralba, 2001, 2006) suggests            luminance properties. Furthermore, no efforts have
that both capture a robust vocabulary of natural scene            been focused on formulating chromatic/luminance
statistics.                                                       categories for real-world scenes (although Oliva &
    A key tenet of the spatial envelope model is that             Schyns [2000] did use color histograms to examine
humans compute an intermediate representation of 3D               the diagnosticity of color between different semantic
spatial structure, which is in turn used to infer semantic        categories).
category during early visual processing (Greene &                    The open endedness (i.e., multidimensionality) of
Oliva, 2006, 2009a, 2009b, 2010; Oliva, 2005; Oliva               the 2D appearance task instructions may explain the
& Torralba, 2001, 2006; Torralba & Oliva, 2002). In               greater disagreement between observers relative to the
support of this model, previous work has shown that               semantic task (Figure 4), although it does not explain
scene structure is extracted from natural scenes before           why agreement was higher than for the 3D spatial
semantic categories are accessed (Greene & Oliva,                 structure task (which was more constrained). Either
2009a), and that humans use spatial structure cues to             way, it is entirely possible that different observers
inform judgements of semantic category (Greene &                  were grouping images based on different feature
Oliva, 2009b, 2010). Although we did not manipulate               dimensions—a problem that highlights the importance
presentation duration directly, in Experiment 2, we               of carefully designing and standardizing observer
did find that a classifier trained to predict semantic            instructions.
category from 3D spatial structure category produced                 Various characteristics of the sorting task in
reasonable results: 57.36% accuracy on all images,                Experiment 1 may undesirably bias human behavior
and 68.33% accuracy on only the typical category                  away from natural categorization. The task instructions,
exemplars. However, in Experiment 3, we also found                number of images, constraints on the number of
that, human-rated spatial envelope properties are poor            images allowed per category (more than one), and
predictors of semantic categories (45.76% correct).               the range of permitted categories (three to 10) may
The reduced discriminative power of spatial envelope              influence observer sorting patterns. While it is difficult
properties (compared with our spatial structure                   to conduct a categorization experiment with no
categories, and other classification results; e.g., Greene        constraints on behavior, it would be beneficial for future
& Oliva, 2009b) may be due to the taxonomical                     work to investigate how various task demands bias
structure of our empirically derived semantic category            categorization.
system. Perhaps the impressive performance of
previously reported spatial envelope-driven semantic
classification (e.g., Greene & Oliva, 2009b) is produced,
in part, by the selection of semantic categories that             Estimating spatial envelope properties using
are discriminable based on spatial envelope profiles              cluster weighted models
(i.e., spectral signatures; see Oliva & Torralba, 2001).
It is also possible that this result is caused by an                 Ross and Oliva (2010) previously suggested that
idiosyncratic set of SYNS categories. Future research             CWMs are “well suited to encoding structural scene
should examine whether empirically derived category               priors” (pp. 21). Yet, in Experiment 3, we showed that
systems from other datasets also produce a weak                   the relationship between low-level GIST features and
association between spatial envelope properties and               spatial envelope properties—a relationship encoded
semantic content. Or, perhaps a simpler explanation               by the proposed CWMs—varied with the chosen
exists: prior studies testing semantic classification             dataset. We demonstrated that models trained on
from spatial envelope properties have used up to seven            Ross & Oliva’s dataset produce inaccurate estimations
properties (Greene & Oliva, 2006, 2009b), while we                of spatial envelope properties in the SYNS dataset.
used three (used by Ross & Oliva, 2010). We would                 Similarly, the optimal spatial resolutions for estimating
likely see an improvement in classification performance           spatial envelope properties varied between the datasets.
if we used additional properties like “navigability” and          Although the cause of this dataset-dependency is
“temperature” (Greene & Oliva, 2006, 2009b).                      unclear, it is conceivable that the perception of mean
    Although observers were instructed to sort images             depth, openness and perspective co-vary with the
based on multiple, complex, feature dimensions,                   photographic field of view, that is, the focal length
including “patterns,” “textures,” and “color,” our 2D             of the camera, which will determine the amount of
appearance categories contain only two distinguishable            perspective apparent in the image. While the field
feature dimensions: color (blue, green, brown) and                of view of the stereoscopic SYNS images we used
Journal of Vision (2021) 21(2):8, 1–31                  Anderson et al.                                                     24

was fixed at 31.12 × 22.36° (Adams et al., 2016), the              that typical category exemplars are easier to classify
spatial envelope literature is based on crowd-sourced              than atypical or randomly sampled images.
photography—images taken from multiple different
cameras, presumably with varying focal lengths.
Spatial perception may also depend upon camera                     The scalability of our clustering method
pose. The SYNS stereo pairs were all taken at eye
height, with a horizontal optical axis. In contrast, the              The categories derived from the SYNS database may
crowd-sourced images used for spatial envelope work                not be suitable for application to all other databases.
vary substantially in camera height and angle. The                 Large-scale image repositories such as Places (Zhou
sensitivity of the GIST representation to low-level                et al., 2014), ImageNet (Deng et al., 2009), and SUN
differences caused by camera properties, or even simpler           (Xiao et al., 2010) have a greater range of environments
changes like modifications to global contrast (which               than SYNS, and while the SYNS dataset was designed
also affects the GIST [Oliva & Torralba, 2001], but has            to maximize environmental variation, some scenes such
no effect on the spatial structure of an image), suggest           as deserts and mountains—that would conceivably
that they may be poor at representing scene layout                 comprise independent categories—are not included, as
information.                                                       they do not occur in the sampled region of southern
   GIST is a popular low-level summary statistic in                England (Adams et al., 2016). In Experiment 2, we
computer vision, yet recent advances in convolutional              tested the generalizability of our category systems on
neural networks (CNNs) has produced better models                  novel images taken from the same locations and using
of spatial structure processing. For example, Cichy                the same camera, with the same focal length, and so on.
et al. (2017) measured the correlation between human               A stronger test of generalization might draw data from
MEG responses to the dimension of scene size (i.e.,                additional image repositories, but this introduces the
the expansiveness of a scene), and the predictions of              problem of applying unsuitable taxonomies to new and
three competing models: GIST, HMAX (a biologically                 different datasets.
inspired hierarchical model; Serre, Wolf, & Poggio,                   To circumvent this problem, in Experiment 4, we
2005), and a CNN trained to classify scenes from the               applied our method to a distinct dataset used in a same–
Places database. The CNN produced layer activations                different psychophysical task, in which participants
that correlated more strongly with human responses                 viewed pairs of images sampled from the SUN database
than the other two models. It is plausible then, that,             and i) judged whether they were drawn from the
with a state-of-the-art model of spatial structure                 same or a different semantic category and ii) typed a
estimation (e.g., an appropriately trained CNN), we                category label for the left image (Greene et al., 2016).
might observe less dataset dependency and stronger                 We found that the categories generated by our method
predictions of spatial structure properties. A thorough            outperformed the SUN taxonomy, and a competing
analysis of how CNN feature representations relate to              clustering method (Greene, 2019), in predicting human
spatial structure categories is beyond the scope of this           same/different judgements, and in capturing variance in
paper, but future research may address this problem.               the meaning of participant-generated image labels.
                                                                      It is worth noting, however, that performance
                                                                   differences were sometimes minor (see Tables 6 and
Typicality enhances category discrimination                        7). In fact, we observed a strikingly high agreement
                                                                   (near-perfect, when the number of clusters is matched)
   Some images or scenes are clearer category members              between the clusters generated by our method,
than others. Prototype theory conceptualizes category              and the SUN taxonomy. This result is impressive
membership as the proximity of an instance to a                    because the SUN taxonomy was developed completely
central exemplar (Rosch, 1999; Rosch & Mervis, 1975).              independently of the experimental data used to derive
Typical category instances have the “most attributes               our categories. The SUN categories were determined
in common with other members of the category and                   by identifying place names represented in WordNet,
[the] least attributes in common with other categories”            collapsing over synonyms, and then using these as
(p. 573; Rosch & Mervis, 1975). Real-world scene                   search-terms in various search engines to retrieve
categorization behavior supports this theory: Torralbo             images (Xiao et al., 2010).
et al., (2013) found that the variance in spatial structure           WordNet organizes words into concepts by grouping
and color for typical images is smaller than atypical              synonyms into sets termed synsets. These synsets are
images. Typical category exemplars are categorized                 structured hierarchically—a design decision inspired
more efficiently than atypical images (Torralbo et al.,            by early investigations of semantic memory (Collins
2013), and classifiers trained on global image features            & Loftus, 1975; Miller, 1990). Expert lexicographers
achieve greater accuracy for typical category exemplars            generated these synsets manually. Consequently, the
(Ehinger et al., 2011). Our findings are consistent with           similarity between the category systems produced by the
these results. In Experiments 2 and 3, we demonstrated             “WordNet approach” and our data-driven approach,
Journal of Vision (2021) 21(2):8, 1–31                 Anderson et al.                                                     25

may reflect the universality of how category systems are          Stairs, Natural,” based on the environmental context.
represented by humans, lexicographers and psychology              By contrast, Greene’s method simply reproduced
participants alike. Moreover, these results suggest that          the original SUN category (see Figure 13). Despite
linguistic taxonomies generalize to visual scenes—a               these differences, both clustering methods frequently
finding consistent with research showing that long-term           produced identical categories (Figure 13, green boxes),
semantic memory is modality independent (Coccia,                  and can be used for different purposes: our method
Bartolini, Luzzi, Provinciali, & Lambon Ralph, 2004;              can be used to derive clusterings in the absence of any
Simanova, Hagoort, Oostenveld, & Van Gerven,                      assumptions about the taxonomical structure of the
2014).                                                            dataset; Greene’s (2019) method can be applied as an
   One prominent difference between the SUN                       inexpensive method of simplifying and refining existing
taxonomy and our category systems is the number of                category systems.
categories. Using k-fold cross-validation, we found
that the optimal number of categories for 1,000 and
712 images was 35 and 24, respectively. The SUN                   The limits of our clustering method
taxonomy has more than twice this number (72 and
55 categories for 1,000 and 712 images, respectively).                Our method produced reasonable clusterings for
Our method generated simpler category systems                     a sparse dataset (see Figure 13) in which more than
with a larger number of images per category. This                 90% of the datapoints were missing. Moreover, as
difference may reflect the fine-grained differentiation           reported in the Supplementary Materials, we tested our
between different word meanings in WordNet. Humans                clustering method on simulated data, and compare
show substantial disagreement regarding the meaning               the results against two alternative methods (k-medoids
denoted by different WordNet Synsets (Chklovski &                 and spectral clustering). We found that our method
Mihalcea, 2003), and, in the NLP literature, merging              was more robust against high levels (50%) of response
synsets into simpler taxonomies improves word-sense               noise. We also tested the behavior of our method under
disambiguation (Navigli, 2006; Snow et al., 2007).                conditions of high interparticipant disagreement, and
   In terms of human behavior, the preference for                 found that our method produced the correct number of
coarse-grained taxonomies may relate to basic-level               clusters even when interparticipant disagreement was
categorization. Humans show a reliable bias toward                as high as 25%. Thus, our method can be safely applied
categorizing stimuli (visual and nonvisual) at the basic          to experimental data: i) containing a large amount of
level (Hajibayova, 2013; Rosch, 1999; Rosch & Lloyd,              missing data, ii) with high levels of response noise, and
1978; Rosch & Mervis, 1975; Rosch, Mervis, Gray,                  iii) collected from a heterogenous population, where
Johnson, & Boyesbraem, 1976; Tversky & Hemenway,                  interparticipant agreement may be low.
1983), and previous work has demonstrated that                        We tested our method on 80, 712, and 1,000 images,
visual scenes involuntarily activate basic-level semantic         but many large-scale databases contain hundreds of
concepts (Greene & Li, 2014). An inspection of the                thousands or even millions of images. The sorting
category labels assigned to our categories supports this          task in Experiment 1 works well for a small number
explanation: the SUN gatehouse category is labelled               of images (in our case, 80), but with larger sets of
“castle” and the outdoor newsstand is labelled “shop”             images, the workspace would quickly become cluttered
(see Figure 13). Because gatehouses are typically                 and unmanageable. A physical sorting task, where
enclosed within the grounds of castles and outdoor                participants arrange pictures of scenes in a large, open
newsstands are a subtype of shop, participants seem               space, might fare better, but this comes with its own
to be collapsing over more fine-grained categories.               limitations (e.g., error-prone manual data entry, time
Similarly, the SYNS semantic category system                      consuming to run).
derived in Experiment 1 is mostly comprised of basic                  The same–different task described in Experiment 4
level-categories, with the exception of the superordinate         may seem better, but the number of judgements
“Nature” category. Taken together, our findings suggest           needed to “fill” a similarity/confusion matrix increases
that humans represent large numbers of visual scenes              quadratically with the number of images—a fact
using a relatively small set of coarse-grained categories.        highlighted by Greene et al. (2016), who recruited more
   Greene’s (2019) category systems showed weaker                 than 2000 participants, and only managed to collect
agreement with the SUN taxonomy and the categories                data for 0.27% of the possible image combinations (of
generated by our method (see Figure 12). This result              42,000 images).
may relate to the constraints within Greene’s clustering              Assuming a full, large-scale dataset can be practically
method: SUN categories can be eliminated or merged,               collected, an additional limiting factor is computational
but new categories cannot be created by dividing                  efficiency. Because the number of possible clusterings
SUN categories into smaller units. For example, our               given n stimuli and k clusters is kn , the search space
method produced two separate categories: “Forest” and             grows rapidly as the dataset size increases. In the
“Mountain” for the single SUN category: “Flight of                Supplementary Materials, we examine the efficiency
Journal of Vision (2021) 21(2):8, 1–31                   Anderson et al.                                                                   26

of our method as a function of the number of stimuli
(n), and number of clusters (k), and show that, while                   Conclusion
runtime increases with both these variables, our method
is still computationally feasible for large datasets (albeit           Scene understanding is commonly measured
slow when n and k are large, for example, n = 50,000,               by assessing categorization behavior, but these
k = 500).                                                           measurements will only be useful if the right category
   Our method is also not limited to the domain of                  system is used. We have proposed a novel method for
scene categorization: it can be applied to data collected           generating participant-driven category systems. Using
from any psychophysical experiment that yields                      stereoscopic images of real-world scenes from the
similarity judgements between pairs of stimuli. For                 SYNS database (Adams et al., 2016), we established
example, our method could be used to derive object,                 ground-truth categories across three dimensions
color, and texture categories, and could be applied to              (semantics, 3D spatial structure, 2D appearance). We
other modalities to investigate auditory, tactile, and              explored some basic characteristics of our categories,
olfactory processing.                                               and presented results that suggest color and spatial
                                                                    structure provide intermediate representations useful
                                                                    for determining semantic category. We then tested our
                                                                    method on a larger dataset, and observed a superior
Further questions regarding the utility of                          agreement with human judgements than rival category
categories                                                          systems, but also a surprising degree of agreement
                                                                    between our clusterings, and the categories represented
   Although it is evident that humans use categorical               in the SUN taxonomy. Further simulations revealed
descriptions in everyday life to communicate notions                that our method is robust against response noise and
of place or location using labels like “Beach” and                  participant heterogeneity. This method may be useful
“Residential,” categorical representations do not                   for creating and/or evaluating class label systems
capture intracategory variations. Further, we have                  for existing databases and for investigating specific
assumed that any given image must belong to exactly                 hypotheses regarding the organization of categorical
one category within a category system, whereas it                   constructs.
may be more natural to allow images to belong to                      Keywords: high-level scene perception, scene
multiple categories (Patterson, Xu, Su, & Hays, 2014).              categorization, clustering
For example, a scene of a house on the seashore may
belong to both “Beach” and “Residential” categories.
In contrast, attributes (e.g., materials or functions) can
traverse category boundaries and capture intracategory
variation. Attribute perception may complement                          Acknowledgments
category representations by providing the fine-grained
information that categories lack (Ferrari & Zisserman,
2008).                                                                Supported by EPSRC grant EP/K005952/1, EPSRC
   Nativist approaches to category systems posit                    grant EP/S016368/1, and a York University VISTA
that categorization behavior reflects a universal taxa              Visiting Trainee Award.
of perceptual ordering (Berlin & Kay, 1991; Rosch                   Commercial relationships: none.
& Lloyd, 1978). Other investigators have stressed                   Corresponding author: Matt D. Anderson.
that labelling systems vary to a large extent across                Email: matt.anderson@soton.ac.uk.
individuals and cultures (Hajibayova, 2013; Levelt,                 Address: Room 4127, B44 University Road, University
2014). For example, highly familiar category instances              of Southampton, Southampton, SO17 1PS, UK.
(e.g., to a Neapolitan, Mt Vesuvius may be a familiar
instance of “mountain”) are accessed at the individual,
rather than the categorical level (Anaki & Bentin,
2009). Personal expertise may therefore determine
whether a scene is identified categorically or not. This                Footnotes
factor casts doubt over the generality of not only our
categorization system, but fixed categorical taxonomies             1
                                                                      The fact that power spectra can be encoded efficiently, and discriminate
in general. Future research will benefit from assessing             semantic categories well, does not mean they are necessarily used by
how individual, geographical and cultural variables                 humans. Indeed, humans process the presence or absence of animals in
shape psychological category representations (Nisbett               natural scenes without exploiting the information available from the power
                                                                    spectrum (Wichmann, Drewes, Rosas, & Gegenfurtner, 2010).
& Masuda, 2006). Our category formation method                      2
                                                                      We use the standard Rand index for coordinate ascent, and the ARI for
could serve as a useful tool for investigating these                cross-validation because, for S, the number of categories is undefined
problems.                                                           and has to be approximated by c. For c and the categories generated by
Journal of Vision (2021) 21(2):8, 1–31                                  Anderson et al.                                                     27

the left-out subject, the number of categories is defined, so ARI is the               human brain revealed by magnetoencephalography
preferred metric.                                                                      and deep neural networks. Neuroimage, 153,
3
  Ross and Oliva (2010) additionally instructed subjects to perform a
natural vs. man-made categorization, but, because the semantic category
                                                                                       346–358, doi:10.1016/j.neuroimage.2016.03.063.
system from Experiment 1 challenges the assumption of a sharp division             Coccia, M., Bartolini, M., Luzzi, S., Provinciali, L., &
between superordinate categories and finer categorical representations, we             Lambon Ralph, M. A. (2004). Semantic memory
omitted this judgement.
4
  In Experiment 1, participants assigned labels to whole categories, whereas           is an amodal, dynamic system: Evidence from the
in Experiment 4 they were assigned to single images. Therefore, we cannot              interaction of naming and object use in semantic
use the same method as in Experiment 1.                                                dementia. Cognitive Neuropsychology, 21(5),
                                                                                       513–527.
                                                                                   Collin, C. A. (2006). Spatial-frequency thresh-
                                                                                       olds for object categorisation at basic and
 References                                                                            subordinate levels. Perception, 35(1), 41–52,
                                                                                       doi:10.1068/p5445.
Adams, W. J., Elder, J. H., Graf, E. W., Leyland,                                  Collin, C. A., & McMullen, P. A. (2005). Subordinate-
    J., Lugtigheid, A. J., & Muryy, A. (2016). The                                     level categorization relies on high spatial frequencies
    Southampton-York Natural Scenes (SYNS) dataset:                                    to a greater degree than basic-level categorization.
    Statistics of surface attitude. Scientific Reports, 6,                             Perception & Psychophysics, 67(2), 354–364,
    35805, doi:10.1038/srep35805.                                                      doi:10.3758/bf03206498.
Anaki, D., & Bentin, S. (2009). Familiarity                                        Collins, A. M., & Loftus, E. F. (1975). A spreading-
    Effects on Categorization Levels of Faces                                          activation theory of semantic processing.
    and Objects. Cognition, 111(1), 144–149,                                           Psychological Review, 82(6), 407.
    doi:10.1016/j.cognition.2009.01.002.                                           Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Li, F.
Banno, H., & Saiki, J. (2015). The processing speed                                    F., & IEEE. (2009, June). ImageNet: A large-scale
    of scene categorization at multiple levels of                                      hierarchical image database. Paper presented at the
    description: The superordinate advantage revisited.                                IEEE Computer Vision and Pattern Recognition,
    Perception, 44(3), 269–288, doi:10.1068/p7683.                                     Miami Beach, Florida.
Bentin, S., & Golland, Y. (2002). Meaningful                                       Ehinger, K. A., Xiao, J., Torralba, A., & Oliva, A.
    processing of meaningless stimuli: The influence                                   (2011). Estimating scene typicality from human
    of perceptual experience on early visual                                           ratings and image features. Perception, 40 (1),
    processing of faces. Cognition, 86(1), B1–B14,                                     91–914.
    doi:10.1016/s0010-0277(02)00124-5.                                             Fabre-Thorpe, M. (2011). The characteristics and
Berlin, B., & Kay, P. (1991). Basic Color Terms: Their                                 limits of rapid visual categorization. Frontiers in
    Universality and Evolution. Berkeley: University of                                Psychology, 2, 243, doi:10.3389/fpsyg.2011.00243.
    California Press.                                                              Fei-Fei, L., Iyer, A., Koch, C., & Perona, P. (2007). What
Boutonnet, B., & Lupyan, G. (2015). Words jump-start                                   do we perceive in a glance of a real-world scene?
    vision: A label advantage in object recognition.                                   Journal of Vision, 7(1), 10, doi:10.1167/7.1.10.
    Journal of Neuroscience, 35(25), 9329–9335.                                    Fei-Fei, L., & Perona, P. (2005, June). A Bayesian
Bulthoff, H. H., & Mallot, H. A. (1988). Integration                                   hierarchical model for learning natural scene
    of depth modules - stereo and shading. Journal                                     categories. Paper presented at the Conference on
    of the Optical Society of America: Optics                                          Computer Vision and Pattern Recognition, San
    Image Science and Vision, 5(10), 1749–1758,                                        Diego, California.
    doi:10.1364/josaa.5.001749.                                                    Ferrari, V., & Zisserman, A. (2008, December).
Castelhano, M. S., & Henderson, J. M. (2008). The                                      Learning visual attributes. Paper presented at the
    influence of color on the perception of scene                                      Advances in neural information processing systems,
    gist. Journal of Experimental Psychology–Human                                     Vancouver, British Columbia, Canada.
    Perception and Performance, 34(3), 660–675,                                    Goffaux, V., Jacques, C., Mouraux, A., Oliva, A.,
    doi:10.1037/0096-1523.34.3.660.                                                    Schyns, P. G., & Rossion, B. (2005). Diagnostic
Chklovski, T., & Mihalcea, R. (2003, October).                                         colours contribute to the early stages of scene
    Exploiting agreement and disagreement of human                                     categorization: Behavioural and neurophysiological
    annotators for word sense disambiguation. Paper                                    evidence. Visual Cognition, 12(6), 878–892,
    presented at the Proceedings of Recent Advances in                                 doi:10.1080/13506280444000562.
    NLP (RANLP 2003), Borovets, Bulgaria.                                          Goffaux, V., Jemel, B., Jacques, C., Rossion, B.,
Cichy, R. M., Khosla, A., Pantazis, D., & Oliva, A.                                    & Schyns, P. G. (2003). ERP evidence for task
    (2017). Dynamics of scene representations in the                                   modulations on face perceptual processing at
Journal of Vision (2021) 21(2):8, 1–31               Anderson et al.                                                   28

   different spatial scales. Cognitive Science, 27(2),              of object and spatial layout information. Cerebral
   313–325, doi:10.1016/s0364-0213(03)00002-8.                      Cortex, 23(4), 947–957.
Greene, M. R. (2013). Statistics of high-level                  Hildreth, E. C. (1987). The computational study of
   scene context. Frontiers in Psychology, 4, 777,                  vision. In Advances in Physiological Research (pp.
   doi:10.3389/fpsyg.2013.00777.                                    203–231). New York: Springer.
Greene, M. R. (2019). The information content of                Hubert, L., & Arabie, P. (1985). Comparing partitions.
   scene categories. In Psychology of Learning and                  Journal of Classification, 2(2-3), 193–218,
   Motivation (Vol. 70, pp. 161–194). New York:                     doi:10.1007/bf01908075.
   Elsevier.                                                    Johnson, K. E., & Mervis, C. B. (1997). Effects of
Greene, M. R., Baldassano, C., Esteva, A., Beck, D.                 varying levels of expertise on the basic level
   M., & Li, F. F. (2016). Visual scenes are categorized            of categorization. Journal of Experimental
   by function. Journal of Experimental Psychology-                 Psychology–General, 126(3), 248–277, doi:10.1037/
   General, 145(1), 82–94, doi:10.1037/xge0000129.                  /0096-3445.126.3.248.
Greene, M. R., & Li, F. F. (2014). Visual Categorization        Jolicoeur, P., Gluck, M. A., & Kosslyn, S. M.
   is automatic and obligatory: Evidence from                       (1984). Pictures and names - Making the
   Stroop-like paradigm. Journal of Vision, 14(1) , 14,             connection. Cognitive Psychology, 16(2), 243–275,
   doi:10.1167/14.1.14.                                             doi:10.1016/0010-0285(84)90009-4.
Greene, M. R., & Oliva, A. (2006, July). Natural scene          Joubert, O. R., Rousselet, G. A., Fize, D., &
   categorization from conjunctions of ecological                   Fabre-Thorpe, M. (2007). Processing scene
   global properties. Paper presented at the Proceedings            context: Fast categorization and object
   of the Annual Meeting of the Cognitive Science                   interference. Vision Research, 47(26), 3286–3297,
   Society, Vancouver, British Columbia, Canada.                    doi:10.1016/j.visres.2007.09.013.
Greene, M. R., & Oliva, A. (2009a). The briefest                Kadar, I., & Ben-Shahar, O. (2012). A perceptual
   of glances: The time course of natural scene                     paradigm and psychophysical evidence for hierarchy
   understanding. Psychological Science, 20(4),                     in scene gist processing. Journal of Vision, 12(13) ,
   464–472, doi:10.1111/j.1467-9280.2009.02316.x.                   16, doi:10.1167/12.13.16.
Greene, M. R., & Oliva, A. (2009b). Recognition                 Larson, A. M., Freeman, T. E., Ringer, R. V., &
   of natural scenes from global properties:                        Loschky, L. C. (2014). The spatiotemporal
   Seeing the forest without representing the                       dynamics of scene gist recognition. Journal of
   trees. Cognitive Psychology, 58(2), 137–176,                     Experimental Psychology–Human Perception and
   doi:10.1016/j.cogpsych.2008.06.001.                              Performance, 40(2), 471–487, doi:10.1037/a0034986.
Greene, M. R., & Oliva, A. (2010). High-                        Larson, A. M., & Loschky, L. C. (2009). The
   level aftereffects to global scene properties.                   contributions of central versus peripheral vision
   Journal of Experimental Psychology-Human                         to scene gist recognition. Journal of Vision, 9(10) ,
   Perception and Performance, 36(6), 1430–1442,                    6.1–16, doi:10.1167/9.10.6.
   doi:10.1037/a0019058.                                        Lazebnik, S., Schmid, C., & Ponce, J. (2006, June).
Groen, I. I. A., Greene, M. R., Baldassano, C., Li,                 Beyond bags of features: Spatial pyramid matching
   F. F., Beck, D. M., & Baker, C. I. (2018). Distinct              for recognizing natural scene categories. Paper
   contributions of functional and deep neural                      presented at the 2006 IEEE Computer Society
   network features to representational similarity                  Conference on Computer Vision and Pattern
   of scenes in human brain and behavior. Elife, 7,                 Recognition (CVPR’06), New York, New York.
   e32962, doi:10.7554/eLife.32962.                             Levelt, W. (2014). A History of Psycholinguistics:
Hajibayova, L. (2013). Basic-level categories:                      The Pre-Chomskyan Era. Oxford, UK: Oxford
   A review. Journal of Information Science,                        University Press.
   39(5), 676–687. Retrieved from <Go to                        Li, F. F., VanRullen, R., Koch, C., & Perona, P. (2002).
   WoS>://WOS:000232033700003.                                      Rapid natural scene categorization in the near
Hamm, J. P., & McMullen, P. A. (1998). Effects                      absence of attention. Proceedings of the National
   of orientation on the identification of rotated                  Academy of Sciences, USA, 99(14), 9596–9601,
   objects depend on the level of identity.                         doi:10.1073/pnas.092277599.
   Journal of Experimental Psychology–Human                     Loschky, L. C., & Larson, A. M. (2010). The
   Perception and Performance, 24(2), 413–426,                      natural/man-made distinction is made before
   doi:10.1037//0096-1523.24.2.413.                                 basic-level distinctions in scene gist processing.
Harel, A., Kravitz, D. J., & Baker, C. I. (2012).                   Visual Cognition, 18(4), 513–536, doi:10.1080/
   Deconstructing visual scenes in cortex: Gradients                13506280902937606.
Journal of Vision (2021) 21(2):8, 1–31               Anderson et al.                                                    29

Mack, M. L., & Palmeri, T. J. (2015). The dynamics of               Association for Computational Linguistics, Sydney,
   categorization: Unraveling rapid categorization.                 Australia.
   Journal of Experimental Psychology-General,                  Nisbett, R. E., & Masuda, T. (2006). Culture and Point
   144(3), 551–569, doi:10.1037/a0039184.                           of View. Biological and Cultural Bases of Human
Maier, M., Glage, P., Hohlfeld, A., & Rahman, R.                    Inference (pp. 49–70). Mahwah, NJ: Lawrence
   A. (2014). Does the semantic content of verbal                   Erlbaum Associates. Retrieved from <Go to
   categories influence categorical perception?                     ISI>://WOS:000238833200003.
   An ERP study. Brain and Cognition, 91, 1–10,                 Noorman, S., Neville, D. A., & Simanova, I.
   doi:10.1016/j.bandc.2014.07.008.                                 (2018). Words affect visual perception by
Malcolm, G. L., Groen, I. I. A., & Baker, C. I.                     activating object shape representations. Scientific
   (2016). Making sense of real-world scenes.                       Reports, 8(1), 14156. Retrieved from <Go to
   Trends in Cognitive Sciences, 20(11), 843–856,                   WoS>://WOS:000089294000002.
   doi:10.1016/j.tics.2016.09.003.                              Oliva, A. (2005). Gist of the scene. In Neurobiology of
Malcolm, G. L., Nuthmann, A., & Schyns, P. G.                       attention (pp. 251–256). New York: Elsevier.
   (2014). Beyond gist strategic and incremental                Oliva, A., & Schyns, P. G. (2000). Diagnostic colors
   information accumulation for scene categorization.               mediate scene recognition. Cognitive Psychology,
   Psychological Science, 25(5), 1087–1097,                         41(2), 176–210, doi:10.1006/cogp.1999.0728.
   doi:10.1177/0956797614522816.
                                                                Oliva, A., & Torralba, A. (2001). Modeling the shape
Malt, B. C. (1995). Category coherence in cross-cultural            of the scene: A holistic representation of the spatial
   perspective. Cognitive Psychology, 29(2), 85–148.                envelope. International Journal of Computer Vision,
Marr, D. (1982). Vision: A Computational Investigation              42(3), 145–175, doi:10.1023/a:1011139631724.
   into the Human Representation and Processing of              Oliva, A., & Torralba, A. (2006). Building the gist
   Visual Information. Cambridge, MA: MIT Press.                    of a scene: The role of global image features in
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013).              recognition. Progress in Brain Research, 155, 23–36,
   Efficient estimation of word representations in                  doi:10.1016/s0079-6123(06)55002-2.
   vector space. arXiv preprint arXiv:1301.3781.                Park, S., Brady, T. F., Greene, M. R., & Oliva,
Mikolov, T., Sutskever, I., Chen, K., Corrado, G.                   A. (2011). Disentangling scene content from
   S., & Dean, J. (2013, December). Distributed                     spatial boundary: complementary roles for the
   representations of words and phrases and their                   parahippocampal place area and lateral occipital
   compositionality. Paper presented at the Advances                complex in representing real-world scenes. Journal
   in Neural Information Processing Systems, Lake                   of Neuroscience, 31(4), 1333–1340. Retrieved from
   Tahoe, Nevada.                                                   <Go to WoS>://WOS:000383003100018.
Miller, G. A. (1990). Nouns in WordNet: A lexical               Patterson, G., Xu, C., Su, H., & Hays, J. (2014). The
   inheritance system. International Journal of                     SUN attribute database: Beyond categories for
   Lexicography, 3(4), 245–264.                                     deeper scene understanding. International Journal
Miller, G. A. (1995). Wordnet - A lexical database for              of Computer Vision, 108(1-2), 59–81. Retrieved
   English. Communications of the ACM, 38(11),                      from <Go to WoS>://WOS:000251074900018.
   39–41, doi:10.1145/219717.219748.                            Pennington, J., Socher, R., & Manning, C. D.
Murphy, G. L., & Brownell, H. H. (1985).                            (2014, October). Glove: Global vectors for word
   Category Differentiation in object recognition-                  representation. Paper presented at the Proceedings
   typicality constraints on the basic category                     of the 2014 Conference on Empirical Methods in
   advantage. Journal of Experimental Psychology-                   Natural Language Processing (EMNLP), Doha,
   Learning Memory and Cognition, 11(1), 70–84,                     Qatar.
   doi:10.1037/0278-7393.11.1.70.                               Potter, M. C. (1976). Short-term conceptual memory
Murphy, G. L., & Smith, E. E. (1982). Basic-level                   for pictures. Journal of Experimental Psychology:
   superiority in picture categorization. Journal of                Human Learning and Memory, 2(5), 509. Retrieved
   Verbal Learning and Verbal Behavior, 21(1), 1–20,                from <Go to WoS>://WOS:000321148400159.
   doi:10.1016/s0022-5371(82)90412-1.                           Prass, M., Grimsen, C., Konig, M., & Fahle, M. (2013).
Navigli, R. (2006, July). Meaningful clustering of                  Ultra rapid object categorization: effects of level,
   senses helps boost word sense disambiguation                     animacy and context. PloS One, 8(6), e68051,
   performance. Paper presented at the Proceedings of               doi:10.1371/journal.pone.0068051.
   the 21st International Conference on Computational           Ramkumar, P., Hansen, B. C., Pannasch, S., & Loschky,
   Linguistics and 44th Annual Meeting of the                       L. C. (2016). Visual information representation and
Journal of Vision (2021) 21(2):8, 1–31                Anderson et al.                                                   30

    rapid-scene categorization are simultaneous across               Language Learning (EMNLP-CONLL), Prague,
    cortex: An MEG study. Neuroimage, 134, 295–304,                  Czech Republic.
    doi:10.1016/j.neuroimage.2016.03.027.                        Sofer, I., Crouzet, S. M., & Serre, T. (2015). Explaining
Rand, W. M. (1971). Objective criteria for the evaluation            the timing of natural scene understanding with a
    of clustering methods. Journal of the American                   computational model of perceptual categorization.
    Statistical Association, 66(336), 846–850. Retrieved             PloS Computational Biology, 11(9), e1004456,
    from <Go to WoS>://WOS:A1975AN63700003.                          doi:10.1371/journal.pcbi.1004456.
Rosch, E. (1975). Cognitive representations of                   Sun, Q., Ren, Y. J., Zheng, Y., Sun, M. X., & Zheng,
    semantic categories. Journal of Experimental                     Y. J. (2016). Superordinate level processing has
    Psychology-General, 104(3), 192–233, doi:10.1037/                priority over basic-level processing in scene gist
    /0096-3445.104.3.192.                                            recognition. I-Perception, 7(6), 2041669516681307,
Rosch, E. (1999). Principles of categorization. In                   doi:10.1177/2041669516681307.
    Concepts: Core Readings (Vol. 189). Cambridge,               Szummer, M., & Picard, R. W. (1998, January).
    MA: MIT Press.                                                   Indoor-outdoor image classification. Paper
Rosch, E., & Lloyd, B. B. (1978). Cognition and                      presented at the IEEE International Workshop on
    Categorization. Mahwah, NJ: Lawrence Erlbaum                     Content-Based Access of Image and Video Database,
    Associates.                                                      Bombay, India.
Rosch, E., & Mervis, C. B. (1975). Family                        Tanaka, J. W., & Taylor, M. (1991). Object
    resemblances - Studies in internal structure of                  categories and expertise - Is the basic level
    categories. Cognitive Psychology, 7(4), 573–605,                 in the eye of the beholder? Cognitive Psy-
    doi:10.1016/0010-0285(75)90024-9.                                chology, 23(3), 457–482, doi:10.1016/0010-
                                                                     0285(91)90016-h.
Rosch, E., Mervis, C. B., Gray, W. D., Johnson, D. M.,
    & Boyesbraem, P. (1976). Basic objects in natural            Tong, Z. H., Shi, D. X., Yan, B. Z., & Wei, J.
    categories. Cognitive Psychology, 8(3), 382–439,                 (2017, June). A review of indoor-outdoor scene
    doi:10.1016/0010-0285(76)90013-x.                                classification. Paper presented at the Proceedings of
                                                                     the 2017 2nd International Conference on Control,
Ross, M. G., & Oliva, A. (2010). Estimating perception
                                                                     Automation and Artificial Intelligence, Sanya,
    of scene layout properties from global image
                                                                     China.
    features. Journal of Vision, 10(1), 2.1–25,
    doi:10.1167/10.1.2.                                          Torralba, A., & Oliva, A. (2002). Depth estimation
                                                                     from image structure. IEEE Transactions on
Rousselet, G. A., Joubert, O. R., & Fabre-Thorpe, M.
                                                                     Pattern Analysis and Machine Intelligence, 24(9),
    (2005). How long to get to the "gist" of real-world
                                                                     1226–1238, doi:10.1109/tpami.2002.1033214.
    natural scenes? Visual Cognition, 12(6), 852–877,
    doi:10.1080/13506280444000553.                               Torralba, A., & Oliva, A. (2003). Statistics of natural
                                                                     image categories. Network-Computation in Neural
Schyns, P. G., & Oliva, A. (1999). Dr. Angry
                                                                     Systems, 14(3), 391–412, doi:10.1088/0954-898x/14/
    and Mr. Smile: When categorization flexibly
                                                                     3/302.
    modifies the perception of faces in rapid
    visual presentations. Cognition, 69(3), 243–265,             Torralbo, A., Walther, D. B., Chai, B., Caddigan,
    doi:10.1016/s0010-0277(98)00069-9.                               E., Fei-Fei, L., & Beck, D. M. (2013). Good
                                                                     exemplars of natural scene categories elicit
Serre, T., Wolf, L., & Poggio, T. (2005, June). Object
                                                                     clearer patterns than bad exemplars but not
    recognition with features inspired by visual cortex.
                                                                     greater BOLD activity. PloS One, 8(3), e58594,
    Paper presented at the 2005 IEEE Computer
                                                                     doi:10.1371/journal.pone.0058594.
    Society Conference on Computer Vision and Pattern
    Recognition (CVPR’05), San Diego, California.                Tversky, B., & Hemenway, K. (1983). Categories of
                                                                     environmental scenes. Cognitive Psychology, 15(1),
Simanova, I., Hagoort, P., Oostenveld, R., & Van
                                                                     121–149, doi:10.1016/0010-0285(83)90006-3.
    Gerven, M. A. (2014). Modality-independent
    decoding of semantic information from the                    Vailaya, A., Jain, A., & Zhang, H. J. (1998). On
    human brain. Cerebral Cortex, 24(2), 426–                        image classification: City images vs. landscapes.
    434.                                                             Pattern Recognition, 31(12), 1921–1935,
                                                                     doi:10.1016/s0031-3203(98)00079-x.
Snow, R., Prakash, S., Jurafsky, D., & Ng, A. Y.
    (2007, June). Learning to merge word senses.                 VanRullen, R., & Thorpe, S. J. (2001). The time course
    Paper presented at the Proceedings of the 2007                   of visual processing: From early perception to
    Joint Conference on Empirical Methods in Natural                 decision-making. Journal of Cognitive Neuroscience,
    Language Processing and Computational Natural                    13(4), 454–461, doi:10.1162/08989290152001880.
Journal of Vision (2021) 21(2):8, 1–31                Anderson et al.                                                   31

Watt, R. J. (1990). Visual Processing: Computational,            real scene depicted in each image. Consider the model
   Psychophysical, and Cognitive Research. East                  that you would have to physically build to represent
   Sussex, UK: Psychology Press.                                 each scene. For example, you might decide that some
Wichmann, F. A., Drewes, J., Rosas, P., & Gegenfurtner,          scenes are made of one uniform surface—the ground
   K. R. (2010). Animal detection in natural scenes:             plane.
   Critical features revisited. Journal of Vision, 10(4),
   6–6.                                                          Two-dimensional image appearance
Xiao, J. X., Hays, J., Ehinger, K. A., Oliva, A., &
                                                                    Your task is to group together images that contain
   Torralba, A. (2010, Jun 13-18). SUN database:
                                                                 similarities in their 2D appearance. For example, you
   Large-scale scene recognition from Abbey to
                                                                 might think commonalities in the colors, patterns, or
   Zoo. Paper presented at the IEEE Conference on
                                                                 textures.
   Computer Vision and Pattern Recognition, San
                                                                    It is key to remember that there are no right or wrong
   Francisco, California.
                                                                 judgements—choose whichever image combinations
Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., &               make sense to you. Try not to focus on particular
   Oliva, A. (2014, December). Learning deep features            objects within the images. For example, do not group
   for scene recognition using places database. Paper            images according to whether or not they contain a car
   presented at the Advances in Neural Information               or a person. However, it is possible that your groups or
   Processing Systems, Montreal, Quebec, Canada.                 labels may be influenced by the type of objects within
                                                                 the scenes.
                                                                    To group multiple images together, all you need to
                                                                 do is overlap them in the central workspace in front of
 Appendix                                                        you. Press down the mouse wheel to reverse the order
                                                                 of the images, revealing the hidden ones that have other
Participant instructions                                         images stacked on top of them. To get a better, 3D
                                                                 view of any image, select it with the left mouse button
   You will view a large number of images from different         and simultaneously press down the mouse wheel. We
scenes. Your task is to organize them into groups by             encourage you to view the larger, 3D images when
using the mouse to drag and drop the images. You are             making your grouping judgements. When you have
free to organize the images into between three and               finished moving the images around and you are happy
10 groups. All groups should contain more than one               with your groups, you can press the ‘GROUP’ tab on
image, but they do not have to all match in size.                the bottom of the display. You will see that each image
                                                                 within a group will be shown with the same color frame.
                                                                 This is to help you catch any sorting errors. If any of
How to Group                                                     your groups have a wide, black frame around them,
                                                                 this means that you have created either i) too many
Semantic                                                         categories, ii) too few categories, or iii) not enough
                                                                 images in one or more categories. Please return to the
   Your task is to group the images according to type of
                                                                 ‘SORT’ stage if you see any black frames. You can only
place. Think about the themes across the images. Put
                                                                 manipulate the images when the ‘SORT’ tab on the
together images that share a common place category.
                                                                 bottom of the display is highlighted. Finally, assign
One possible example is the category of ‘mountains.”
                                                                 each of the categories a label/set of labels. You are
In this case, all mountain images would be placed in a
                                                                 limited to one to five labels for each group, and each
single group.
                                                                 group must have a corresponding label. Once this is
                                                                 done, you have completed the task.
Three-dimensional spatial structure
   Your task is to group together images that share
similar 3D properties. Think about the structure of the
