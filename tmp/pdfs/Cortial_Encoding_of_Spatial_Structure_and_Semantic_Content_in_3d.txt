                                                                                                                                         1–12 • The Journal of Neuroscience, February 26, 2025 • 45(9):e2157232024




Behavioral/Cognitive


Cortical Encoding of Spatial Structure and Semantic
Content in 3D Natural Scenes
Riikka Mononen,1,2                           Toni Saarela,3 Jaakko Vallinoja,1,2                                        Maria Olkkonen,3 and             Linda Henriksson1,2
1                                                                                                                                                    2
 Department of Neuroscience and Biomedical Engineering, Aalto University, Espoo FI-00076, Finland, MEG Core, Aalto NeuroImaging,
Aalto University, Espoo FI-00076, Finland, and 3Department of Psychology, University of Helsinki, Helsinki FI-00014, Finland


Our visual system enables us to effortlessly navigate and recognize real-world visual environments. Functional magnetic resonance
imaging (fMRI) studies suggest a network of scene-responsive cortical visual areas, but much less is known about the temporal order
in which different scene properties are analyzed by the human visual system. In this study, we selected a set of 36 full-color natural
scenes that varied in spatial structure and semantic content that our male and female human participants viewed both in 2D and 3D
while we recorded magnetoencephalography (MEG) data. MEG enables tracking of cortical activity in humans at millisecond
timescale. We compared the representational geometry in the MEG responses with predictions based on the scene stimuli using
the representational similarity analysis framework. The representational structure ﬁrst reﬂected the spatial structure in the scenes
in time window 90–125 ms, followed by the semantic content in time window 140–175 ms after stimulus onset. The 3D stereoscopic
viewing of the scenes affected the responses relatively late, from ∼140 ms from stimulus onset. Taken together, our results indicate
that the human visual system rapidly encodes a scene’s spatial structure and suggest that this information is based on monocular
instead of binocular depth cues.
Key words: 3D; depth; human visual system; MEG; scene; spatial structure




      Signiﬁcance Statement
      Our visual system enables us to recognize and navigate our visual surroundings seemingly effortlessly, but what exactly
      happens in our brains remains poorly understood. With the help of time-resolved brain imaging (magnetoencephalography),
      we found that the brain ﬁrst encodes the spatial structure of a scene (e.g., cluttered or navigable) before its semantic content
      (e.g., a car park or farm). Brain imaging studies typically use 2D pictures as stimuli. Here we asked whether binocular dis-
      parity, a depth cue which arises from our two eyes seeing the scene from slightly different angles, aids the coding of the spatial
      structure. Our results suggest that this 3D depth cue plays little role in the rapid, initial sensing of our spatial surroundings.


Introduction                                                                                                             PPA has a central role in scene categorization, that is, in recogniz-
Humans understand complex real-world visual environments                                                                 ing the environment or place that we are in, whereas OPA is crit-
without much eﬀort. Scene perception engages specialized                                                                 ical for visually guided navigation, that is, in processing the spatial
cortical areas that include the occipital place area (OPA; Grill-                                                        structure and navigational aﬀordances of the visible environment
Spector, 2003; Dilks et al., 2013) and the parahippocampal place                                                         (for reviews, see Epstein and Baker, 2019; Dilks et al., 2022). Most
area (PPA; Epstein and Kanwisher, 1998; for a review, see                                                                previous brain imaging studies on scene processing have used
Epstein and Baker, 2019). Recent brain imaging studies suggest                                                           functional magnetic resonance imaging (fMRI), and therefore,
complementary roles in visual scene processing for these regions.                                                        much less is known about the temporal order in which diﬀerent
                                                                                                                         scene properties are analyzed by the human visual system (for
                                                                                                                         reviews, see Epstein and Baker, 2019; Bartnik and Groen, 2023).
Received Nov. 14, 2023; revised Nov. 25, 2024; accepted Dec. 24, 2024.                                                   Disentangling the contributions of diﬀerent scene features in cor-
    Author contributions: R.M., T.S., M.O., and L.H. designed research; R.M. and J.V. performed research; T.S., M.O.,
                                                                                                                         tical processing is challenging (Greene and Hansen, 2020), and
and L.H. contributed unpublished reagents/analytic tools; L.H. analyzed data; L.H. wrote the paper.
    This work was supported by Aalto Brain Center. We thank Aalto NeuroImaging staﬀ, especially Mia Illman and           overall, scene processing is unlikely to happen as a clear temporal
Tuomas Tolvanen, for assistance with the measurements and for technical support, and Matti Stenroos for pro-             cascade from low- to high-level scene properties (Ramkumar
viding scripts and help for performing the spatial whitening of the MEG signals. We acknowledge the compu-               et al., 2016; Groen et al., 2017). Recently, interest has turned to
tational resources provided by the Aalto Science-IT project.                                                             how rapidly navigational features are encoded by the human
    The authors declare no competing ﬁnancial interests.
    Correspondence should be addressed to Linda Henriksson at linda.henriksson@aalto.ﬁ.
                                                                                                                         visual system (Harel et al., 2022; Dwivedi et al., 2024).
    https://doi.org/10.1523/JNEUROSCI.2157-23.2024                                                                           Scenes can be understood and analyzed along many dimen-
Copyright © 2025 the authors                                                                                             sions (Malcolm et al., 2016). Anderson et al. (2021) recently
2 • J. Neurosci., February 26, 2025 • 45(9):e2157232024                                                                                             Mononen et al.   • MEG Responses to 3D Scenes
identiﬁed category systems that humans naturally use to classify                                    vision (tested with Ishihara, 38 plates) and normal stereo vision (tested
real-world scenes along three dimensions: visual appearance,                                        with TNO, 19th edition, threshold value 120 arcsec or lower was required;
spatial structure, and semantic content. In a subsequent beha-                                      Piano et al., 2016). Participants were tested also for stereo anomaly by ask-
vioral study, they studied the temporal order in which these                                        ing them to report the direction of the stereo percept in TNO (is the target
                                                                                                    closer or further from the background). Ethical approval for the research
dimensions are used in scene discrimination, with largely similar
                                                                                                    was obtained from Aalto University Ethics Committee. Participants gave
results for spatial structure and semantic content (Anderson                                        written informed consent before participating in the study.
et al., 2022). In the present study, we address this question
more directly by using magnetoencephalography (MEG), which                                               Scene stimuli and experimental design. The stimulus image set
enables tracking of cortical activity in humans at millisecond                                      (Fig. 1A) consisted of 36 full-color stereo-pairs of natural scenes from
timescale (for a review, see Baillet, 2017).                                                        the Southampton-York Natural Scenes (SYNS) database (Adams et al.,
    Little is known about how the human visual system analyzes                                      2016). The selection of the image set was based on human-generated
real-world 3D environments. Most brain imaging studies on                                           labels for semantic category and spatial structure (Anderson et al.,
scene processing use 2D pictures as visual stimuli. The 3D spatial                                  2020, 2021). The six semantic category labels were nature, residence,
structure of a scene can be inferred from the 2D stimulus images,                                   farm, car park (or commercial), road and beach, of which the ﬁrst four
                                                                                                    were the most frequently selected category labels for our selected set of
but the vivid impression of depth provided by disparity that
                                                                                                    scene images (Fig. 1B). The four spatial structure labels were cluttered
arises from our two eyes seeing the world from slightly diﬀerent                                    (or pointy), closed oﬀ, ﬂat, and navigable routes (or tunnel). The scene
viewpoints is, however, diﬃcult to convey with 2D pictorial cues                                    images also had labels for visual appearance (dark, bright, blue, green,
(Wheatstone, 1838; Vishwanath, 2023). Few previous brain                                            brown; Anderson et al., 2021).
imaging studies have investigated how binocular depth cues in                                            The scene stimuli were presented to a polarization-preserving back-
natural scenes aﬀect cortical responses (Duan et al., 2018, 2021).                                  projection screen with a PROPixx DLP LED projector (VPixx
    In the present study, participants viewed a set of full-color                                   Technologies; resolution, 1,920 × 1,080; refresh rate, 120 Hz, 60 Hz per
natural scenes that varied in spatial structure and semantic con-                                   eye) combined with an active circular polarizer (DepthQ, Lightspeed
tent (Fig. 1). Our study participants viewed the same real-world                                    Design). Participants viewed the screen with passive polarizing glasses
scenes both as 2D images and in 3D as a stereo-pair of images.                                      at a viewing distance of 1.65 m. The size of the projected image was
                                                                                                    49.5 × 32.5 cm, subtending ∼17 × 11 degrees of visual angle. The spectral
Using representational similarity analysis (RSA; Kriegeskorte
                                                                                                    power distributions and chromaticities of the projector primaries were
et al., 2008; Kriegeskorte and Kievit, 2013), we investigate the                                    measured with a Photo Research PR-655 SpectraScan Spectroradiometer.
temporal order in which brain responses reﬂect the spatial struc-                                   Mean luminance of the scene images was set to 37.8 cd/m2. Stimuli
ture and the semantic content of the scenes. Moreover, we eval-                                     were presented using MATLAB (MathWorks) with the Psychtoolbox-3
uate the discriminability of the responses between the 2D and 3D                                    extensions (Brainard, 1997; Pelli, 1997; Kleiner et al., 2007). Stereoscopic
viewing conditions. Our aim was to reveal the temporal order in                                     stimuli were presented with the temporally interleaved stereo mode of
which the spatial structure and the semantic content are encoded                                    Psychtoolbox’s PsychImaging function.
by the human visual system and whether disparity depth cues aid                                          The MEG experiment included three viewing conditions: 2D, 3D, and
in coding the spatial structure.                                                                    reversed-stereo viewing. In the 2D condition, the same image was shown
                                                                                                    to both eyes. In the 3D condition, a stereo image pair was shown to the
                                                                                                    left and right eyes. In the reversed-stereo viewing condition, the right eye
Materials and Methods                                                                               image of the stereo image pair was shown to the left eye and vice versa for
   Participants. Twenty healthy volunteers (10 females; age range,                                  the left eye image.
21–37; 17 right-handed, 1 ambidextrous) participated in this study. All                                  One experimental run consisted of 216 stimulus trials and 14 task
participants had normal or corrected to normal visual acuity, normal color                          trials. Each of the 36 scene images was shown two times in each of the




Figure 1. Scene stimuli. A, A set of 36 scene images were selected from the SYNS database (Adams et al., 2016). B, We selected the scenes so that they vary both in their semantic content and
spatial structure based on previously published behavioral data on these same images (Anderson et al., 2020, 2021). Grayscale indicates the proportion of selected label for each of the 36 scenes,
separately for semantic, spatial structure, and visual appearance categorization tasks (Anderson et al., 2020). C, During the MEG experiment, the scene images were presented for 500 ms with a
2,000 ms interstimulus interval. When the scene was replaced by a question mark, the participant’s task was to respond whether the two previous scenes could have been taken from the
same place.
Mononen et al.   • MEG Responses to 3D Scenes                                                                                                J. Neurosci., February 26, 2025 • 45(9):e2157232024 • 3


three diﬀerent viewing conditions (108 unique stimulus trials). The dura-                           data were spatially whitened based on a covariance matrix estimated
tion of one trial was 2.5 s: 500 ms of the stimulus presentation and 2 s                            from the 50 s rest MEG data collected before the task runs using custom
interstimulus interval (Fig. 1C). Participants were instructed to keep their                        MATLAB code, similarly as done in Kurki et al. (2022). Whitening with
gaze at a red ﬁxation dot throughout the experimental runs and pay                                  dimensionality reduction was applied to reduce redundancy in the signals
attention to the scenes. Between stimulus images, the red ﬁxation dot                               that originates from the MEG sensors having overlapping sensitivity
was shown on a uniform gray background of the same mean luminance                                   proﬁles for the underlying cortical sources and from the MaxFilter prepro-
as the images. During task trials, a question mark was shown instead of                             cessing that reconstructs the sensor signals using truncated series expan-
an image, and the participants’ task was to respond with a ﬁnger lift                               sions. The dimensionality of the data was reduced from 306 (number of
whether the two previous scene images could have been taken from                                    channels) to 66–71, which corresponds to the rank of the data after
the same place. The order of the images, conditions, and task trials                                MaxFilter and ICA processing. Epochs were extracted from the continu-
was randomized and was diﬀerent for each participant. The duration                                  ous data from 200 ms before to 1,000 ms after stimulus onset and were
of one run was (216 + 14) × 2.5 s = 575 s. Each participant completed                               baseline corrected from −200 to 0 ms.
ﬁve experimental runs, except for one participant who completed only
four runs due to feeling unwell. In addition, a 50-s rest run, during which                            Representational similarity analysis and statistical analysis.
participants were instructed to maintain ﬁxation at the ﬁxation dot on                              Representational similarity analysis (RSA; Kriegeskorte et al., 2008)
the gray background, was collected before the task runs. All runs were                              was performed using the RSA Toolbox (Nili et al., 2014) and custom
completed in one measurement session.                                                               MATLAB code. The MEG representational dissimilarity matrices
                                                                                                    (MEG-RDMs) were constructed separately for each time point using
    MEG data acquisition and preprocessing. MEG data were recorded in                               cross-validated Euclidean distance estimates:
a magnetically shielded room with a whole-scalp 306-channel MEG
device (MEGIN Oy) at the MEG Core (Aalto NeuroImaging, Aalto
                                                                                                                             2
                                                                                                                            dEuc,cv (rj , ri ) = (rj − ri )TA (rj − ri )B ,
University School of Science). The device comprises 102 triple-sensor
elements, with one magnetometer and two orthogonal planar gradiom-                                  where i and j refer to two diﬀerent conditions (diﬀerent scene stimuli
eters at each location. The signals were sampled at 1,000 Hz using a                                and/or diﬀerent viewing conditions), rj and ri refer to response patterns
recording passband of 0.03–330 Hz. The position of the participant’s                                for the conditions i and j across the whitened sensor space, and A and B
head with respect to the MEG sensors was tracked throughout the exper-                              refer to two independent splits of the data. Leave-one-run-out cross-
iment using ﬁve head position indicator coils. Horizontal and vertical                              validation folds were used to create unbiased estimates (Walther et al.,
electro-oculograms (EOGs) were recorded with the same passband and                                  2016; Guggenmos et al., 2018; Arbuckle et al., 2019). Results were aver-
sampling rate as applied for the MEG data.                                                          aged across the folds. The dimensions of an RDM were 108 × 108 × 1,201,
    The MEG data were preprocessed using spatiotemporal signal-space                                corresponding to the 36 scenes in three diﬀerent viewing conditions
separation implemented in the MaxFilter software (MEGIN Oy) to sup-                                 and 1,201 time points (Fig. 2A). The three 36 × 36 × 1,201 RDMs
press magnetic interference of external sources and to compensate for                               corresponding to the three diﬀerent viewing conditions were extracted
head movement (Taulu and Simola, 2006). Eyeblink and heartbeat-related                              from this RDM for further analysis (Fig. 2A,B). In addition, the
artifacts were removed by applying independent component analysis                                   eﬀect of the viewing condition (2D, 3D, reversed-stereo) was
(ICA) using the FastICA algorithm (Hyvärinen and Oja, 2000) as imple-                               evaluated from the oﬀ-center diagonals corresponding to the distance
mented in the MNE-Python software package (Gramfort et al., 2013).                                  estimate between the same scene under two diﬀerent viewing conditions
EOG signals were used as a reference to ﬁnd eye-related artifacts. For                              (Fig. 2A).
one participant, EOG signal was not available, and the blink-related com-                               The replicability of the representational structure in the MEG-RDMs
ponents were manually selected. Similarly, components related to heart-                             at each time point was evaluated by comparing each individual partici-
beat were manually identiﬁed based on the topography and time course                                pant’s MEG-RDM with the average of the other participants’
of the IC components. The data were low-pass ﬁltered at 40 Hz using                                 MEG-RDMs using Kendall’s tau-a rank correlation (Nili et al., 2014).
the MNE-Python software package (Gramfort et al., 2013). Next, the                                  The signiﬁcance was tested using signed-rank test across the correlations




Figure 2. MEG-RDMs. A, The discriminability of the 36 scenes was evaluated from the MEG response patterns for each pair in three diﬀerent viewing conditions: 2D, 3D, and reversed-stereo
(depicted with red, blue, and green outlines). The oﬀ-center diagonals (depicted with black lines) correspond to the discriminability of the same scene under two diﬀerent viewing conditions. The
analyses were done separately for each time point and individual, and the results were averaged across participants. For visualization purposes, the representational dissimilarity matrices (RDMs)
are rank transformed (Nili et al., 2014). B, An example MEG-RDM averaged across viewing conditions at 100 ms from stimulus onset is shown. See Movie 1 for time-varying RDM and visualization
using multidimensional scaling.
4 • J. Neurosci., February 26, 2025 • 45(9):e2157232024                                                                                       Mononen et al.   • MEG Responses to 3D Scenes
values, and multiple testing across time points was accounted for by                            averaged across the leave-one-run-out cross-validation folds. The replica-
controlling the false discovery rate (FDR). The average RDM replicability                       bility of and the average scene discriminability in the resulting 2D/
can also be interpreted as the (lower bound of the) noise ceiling (Nili                         3D-generalized MEG-RDMs were compared with results obtained with
et al., 2014).                                                                                  MEG-RDMs constructed from response patterns during 3D viewing.
    Generalization between 2D and 3D viewing conditions was analyzed                                 Candidate model RDMs (Fig. 3) were constructed from the human-
with distance estimates                                                                         generated ratings for visual appearance, spatial structure, and semantic
                                                                                                content (Fig. 1B; Anderson et al., 2021). To derive category labels that
                 2
                dEuc,2D3Dgen (rj , ri ) = (rj2D − ri2D )TA (rj3D − ri3D )B ,                    humans naturally use, Anderson et al. (2021) had participants sort
                                                                                                images from the SYNS dataset (Adams et al., 2016) into discrete catego-
where 2D and 3D refer to the diﬀerent viewing conditions, i and j refer to                      ries separately by the type of place (semantic task), by their 3D depth
two diﬀerent scenes, rj and ri refer to the response patterns for these scenes                  structure (spatial structure task), and by their 2D appearance while
across the whitened sensor space, and A and B refer to independent splits                       ignoring the 3D structure (visual appearance task). They used a data-
of the data based on the experimental runs. The distance estimates were                         driven clustering method to derive the representative category labels




Figure 3. Candidate model RDMs and their multidimensional scaling visualizations for (A) visual appearance, (B) spatial structure, and (C) semantic content. RDMs were constructed from the
human-generated ratings (Fig. 1B) by Anderson et al. (2021). Clusters are named for visualization purposes based on the representative category labels.
Mononen et al.   • MEG Responses to 3D Scenes                                                                        J. Neurosci., February 26, 2025 • 45(9):e2157232024 • 5


for each task. Here we used the ratings from their Experiment 2, where              and the variance shared between two models, e.g., visual and semantic, was
they had validated the category labels with 20 new participants for                 calculated as follows:
each of the three tasks (Anderson et al., 2020, 2021). For visual appear-
ance, the scenes had been categorized as dark, bright, blue, green, or
                                                                                              shared varvis, sem = tau-a2vis + tau-a2sem − tau-a2vis,sem
brown. For spatial structure, the categories were cluttered (or pointy),
closed oﬀ, ﬂat, and navigable routes (or tunnel). The semantic category                                            − shared varvis,spa,sen.
labels were nature, residence, farm, car park, road, and beach. We con-
structed the RDMs separately for visual appearance, spatial structure,              The MEG-RDMs were also compared with the average behavior-RDM
and semantic content based on the Euclidean distance between the cat-               using Kendall’s tau-a rank correlation similarly to how the comparison
egory ratings of each pair of scenes. Figure 3 shows these candidate                was done with the other candidate models.
model RDMs and their visualizations using multidimensional scaling
(metric stress).
                                                                                       Data availability. Behavioral-RDMs, MEG-RDMs, candidate model
    MEG-RDMs were also compared with predictions based on low-level
                                                                                    RDMs, and the scene stimuli are uploaded to the Open Science
visual features as captured by the GIST descriptor (Oliva and Torralba,
                                                                                    Foundation repository at https://osf.io/jp26k/.
2001), mean depth from LiDAR data provided with the SYNS dataset
(Adams et al., 2016), and mean chromaticity of each scene in CIE1931
x–y coordinates. MEG-RDMs were compared with the candidate model                    Results
RDMs using Kendall’s tau-a rank correlation. The relatedness was tested
using signed-rank test across the single-subject RDM correlations.
                                                                                    Behavioral scene similarity judgments emphasize semantic
Multiple testing across time points was accounted for by controlling                content
the FDR.                                                                            We selected 36 natural scenes from the SYNS database (Adams
    Multiple candidate models were ﬁtted together using non-negative                et al., 2016) as the stimuli for the MEG experiment (Fig. 1A).
least-squares regression (Khaligh-Razavi and Kriegeskorte, 2014). The               The selection of the scenes was based on human-generated labels
unique contribution of a candidate model was evaluated by comparing                 for semantic category and spatial structure (Anderson et al.,
the explained variance (tau-a2) of the full combined model to the explained         2021). We aimed for a scene stimulus set that varies both
variance of the model, where the candidate model had been left out                  in semantic content and spatial structure (Fig. 1B; e.g., a naviga-
(Khaligh-Razavi and Kriegeskorte, 2014; Henriksson et al., 2019).                   ble nature scene, a ﬂat nature scene, a ﬂat residential scene).
The ﬁtting was cross-validated using a leave-one-participant-out                    The scene images also had labels for visual appearance (dark,
approach.
                                                                                    bright, blue, green, brown; Anderson et al., 2021). First, we
    Behavioral data acquisition and analysis. After the MEG measure-                ask to what extent our study participants used these dimensions
ment, each participant completed a similarity judgment task for the 36              when their behavioral task was to arrange the scenes by their sim-
scene images using a multi-arrangement task (Kriegeskorte and Mur,                  ilarity (Fig. 4A). The scene similarity judgments were collected
2012). The images were presented on a computer screen, and partici-                 after the MEG experiment, and the participants were not given
pants were asked to spatially arrange the images based on their similarity          any speciﬁc instructions on what features to pay attention to.
in a circular arena. They were not given any speciﬁc instructions on                    Figure 4B shows the group-averaged representational dissim-
what features to pay attention to when arranging the scenes. The ﬁrst               ilarity matrix (RDM) based on the behavioral scene similarity
image set included all 36 images. Following trials included subsets of
                                                                                    judgments. The candidate model RDM based on the semantic
the full image set based on the adaptive algorithm for eﬃcient acquisition
of the large number of pairwise dissimilarity judgments (Kriegeskorte               content ratings (Fig. 3C) best explained the behavioral RDM
and Mur, 2012). The task duration was limited to 15 min. Participants               (mean rank correlation coeﬃcient tau-a of 0.26; p < 0.001, one-
completed on average 10 trials (range, 6–20) and spent on average                   tailed signed rank test across the 20 participants; Fig. 4C). In
5 min (range, 3–8.5 min) to arrange the complete set of images on the               other words, our participants’ judgments on the scene similarity
ﬁrst trial.                                                                         emphasized the semantic categories: nature, residence, farm, and
    For each participant, the representational dissimilarity matrix con-            car park. The second-best model was based on the visual appear-
structed based on the scene similarity judgments (behavior-RDM) was                 ance (tau-a = 0.19; p < 0.001), and the spatial structure had the
compared with the candidate model-RDMs for visual appearance, spatial               worst model ﬁt (tau-a = 0.06; p < 0.001; Fig. 4C).
structure, and semantic content using Kendall’s tau-a rank correlation.                 The unique contribution of each of the models in explaining
The relatedness of a candidate model-RDM and behavior-RDM was tested
                                                                                    the behavioral data was evaluated using commonality analysis
using signed-rank test across the single-subject correlations. The upper and
lower bounds of the noise ceiling were estimated using the RSA toolbox              (Lescroart et al., 2015; Tarhan et al., 2021). The semantic content
(Nili et al., 2014). The unique and shared contributions of the models in           accounted for most of the unique variance (tau-a2 = 0.045;
explaining the behavior-RDM were evaluated using commonality analysis               Fig. 4D), the visual appearance accounted for some of the unique
(Lescroart et al., 2015; Tarhan et al., 2021). Non-negative least-squares           variance (tau-a2 = 0.010), and these models also shared some var-
regression was used to ﬁt the models, and the result was cross-validated            iance (tau-a2 = 0.020). The spatial structure did not account for
across subjects. The unique contribution of a model, here as an example             any unique variance.
for the visual model, was calculated as follows:                                        Taken together, our study participants used the semantic and
                                                                                    visual appearance dimensions when judging the scene similarity.
                   unique varvis = tau-a2vis,spa,sem − tau-a2spa,sem ,
                                                                                    Time-resolved response discriminability for the scene stimuli
where tau-a2vis,spa,sem is the explained variance of the full model and             in the MEG data
tau-a2spa,sem is the explained variance of the model, where the visual model        We recorded MEG data from the participants while they viewed
has been left out. Variance shared by all three models was calculated as fol-       the 36 scene stimuli during 2D, 3D, and reversed-stereo viewing
lows:                                                                               conditions. A scene image evokes a complex MEG response that
                                                                                    is diﬃcult to interpret directly. Therefore, we applied representa-
shared varvis,spa,sem = tau-a2vis + tau-a2spa + tau-a2sem − 2 · tau-a2vis,spa,sem   tional similarity analysis (RSA; Kriegeskorte et al., 2008) to cap-
                                                                                    ture the relative distinctiveness of the responses between the
                             + unique varvis + unique varspa + unique varsem ,      scene stimuli.
6 • J. Neurosci., February 26, 2025 • 45(9):e2157232024                                                                                          Mononen et al.   • MEG Responses to 3D Scenes




Figure 4. Behavioral task: scene similarity judgments. A, Participants judged the similarity of the 36 scenes by arranging them in a circular arena (Kriegeskorte and Mur, 2012). This task was
completed after the MEG data collection. B, A representational dissimilarity matrix (RDM) was constructed based on the similarity judgments (Behavior-RDM). Here, the group-average result is
shown. C, Rank correlations between the behavioral RDM and the candidate model RDMs for visual appearance, spatial structure, and semantic content are shown. Blue dots show the individual
data (N = 20), and gray horizontal bar indicates the noise ceiling. D, The unique and shared variance of the behavioral data explained by the combinations of the three models are shown. Venn
diagram illustrates the meaning of each color.


                                                                                                                                                 (peak at ∼110 ms; Fig. 5A). The
                                                                                                                                                 results were highly similar
                                                                                                                                                 between 2D, 3D, and reversed-
                                                                                                                                                 stereo     viewing     conditions
                                                                                                                                                 (Fig. 5B).
                                                                                                                                                     Furthermore,     an      RDM
                                                                                                                                                 (Fig. 2B) captures which scenes
                                                                                                                                                 are represented more similarly
                                                                                                                                                 and which have more distinct rep-
                                                                                                                                                 resentations. The dynamic repre-
                                                                                                                                                 sentational structure in the
                                                                                                                                                 group-average MEG-RDM is
                                                                                                                                                 shown in Movie 1. We can study
                                                                                                                                                 the reliability of the emerging
                                                                                                                                                 structure in the MEG-RDMs by
                                                                                                                                                 comparing the RDMs computed
                                                                                                                                                 for individual participants. The
                                                                                                                                                 MEG-RDMs showed replicable
                                                                                                                                                 structure between participants
                                                                                                                                                 from ∼70 ms from stimulus onset
                                                                                                                                                 (Fig. 5C; peak at ∼100 ms). The
                                                                                                                                                 results were highly similar in the
                                                                                                                                                 three      viewing     conditions
                                                                                                                                                 (Fig. 5D). Figure 5C shows the
                                                                                                                                                 average RDM replicability across
Figure 5. Time-resolved scene decoding and RDM replicability. A, The time course of average response discriminability is shown. Shaded
region indicates the standard-error-of-the-mean (SEM) across participants. Signiﬁcant time points are indicated with red (FDR of 0.01; p viewing conditions, where, for
values computed with one-tailed signed rank test across the 20 participants, FDR adjusted across time points). The black bar indicates the example, an individual partici-
stimulus on-period (0–500 ms). B, Response discriminability results are shown separately for the 2D (shown in red), 3D (blue), and pant’s MEG-RDM during 2D
reversed-stereoscopic (green) viewing conditions. C, The replicability of the representational structure as calculated from the rank correlation viewing was compared with the
between RDMs from diﬀerent participants is shown. Replicability was assessed both within the viewing conditions (shown in black) and average of the other participants’
across the viewing conditions (shown in red). Signiﬁcant time points are indicated by black and red lines correspondingly (FDR of 0.01; p MEG-RDMs during 3D viewing,
values computed with one-tailed signed rank test across the 20 participants, FDR adjusted across time points; stimulus onset at 0 ms). There and this was repeated for all com-
is no signiﬁcant diﬀerence between the across and within RDM replicabilities. D, RDM replicability is shown separately for the three viewing binations of the viewing condi-
conditions in diﬀerent colors.                                                                                                                   tions. There was no signiﬁcant
                                                                                                                                                 diﬀerence between the across and
     The MEG-RDMs (Fig. 2) were constructed based on cross-                                            within RDM replicabilities, and hence, when we next compare
validated Euclidean distance between the evoked responses of                                           the MEG-RDMs with diﬀerent model predictions, the results for
each pair of the scene stimuli. In each cell of an RDM, a system-                                      diﬀerent viewing conditions are averaged.
atically positive distance estimate indicates reliable diﬀerence
between the response patterns corresponding to the two scenes.                                         Spatial structure and semantic content explain
To evaluate how well the MEG signals discriminated between                                             complementary parts of the MEG-RDMs
the scene stimuli, we averaged all pairwise distances in the lower                                     Next, we aim to interpret the emerging structure in the
triangular of each RDM. On average, the scene stimuli elicited                                         MEG-RDMs by comparing them with candidate model RDMs
distinct response patterns from ∼70 ms from the stimulus onset                                         based on the visual appearance, spatial structure, and semantic
Mononen et al.   • MEG Responses to 3D Scenes                                                                                    J. Neurosci., February 26, 2025 • 45(9):e2157232024 • 7




Movie 1.    Time-varying MEG-RDM. The dynamic representational structure in the group-average MEG-RDM is shown and visualized using multidimensional scaling. [View online]


content of the scene stimuli (Fig. 3). Figure 6 shows the average                          participants’ scene similarity judgments were well captured by
rank correlation between each model and the MEG-RDMs. All                                  these dimensions (Fig. 4), and the behavior-RDM correlated
models showed signiﬁcant correlation with the MEG-RDMs                                     with these model-RDMs (Fig. 7E). When ﬁtted to the
(onset ∼80 ms from stimulus onset). The best model ﬁt (peak                                MEG-RDMs together, the average behavior-RDM did not
at ∼110 ms from stimulus onset) was obtained with the spatial                              explain any unique variance that would not have been explained
structure model, which was constructed based on human ratings                              by the visual appearance and semantic content RDMs.
for category labels: cluttered, closed oﬀ, ﬂat, and navigable routes                           Taken together, the MEG-RDMs ﬁrst reﬂected dimensions
(Anderson et al., 2021).                                                                   related to the spatial structure of the scenes followed by the
    The unique contribution of each of the three candidate mod-                            semantic content and perceptual similarity.
els was evaluated by ﬁtting the models to the MEG-RDMs
together and evaluating whether including a model signiﬁcantly                             Generalization across 2D and 3D viewing conditions
improved the ﬁt (Khaligh-Razavi and Kriegeskorte, 2014;                                    As our participants viewed the scenes under diﬀerent viewing
Henriksson et al., 2019). The results are shown in Figure 6D,E.                            conditions, we can ask how well our results generalize across
The candidate model based on the spatial structure had a unique                            the viewing condition and if there is any superiority in the 3D
contribution to the MEG-RDM in time window 90–125 ms, fol-                                 viewing condition. To ask the question whether the diﬀerences
lowed by a unique contribution of the semantic content in time                             in response patterns to a pair of scenes during 2D viewing gener-
window 140–175 ms. The visual appearance model did not                                     alize to 3D viewing, we built the 2D/3D generalized MEG-RDMs
explain any unique variance.                                                               by cross-validating the distance estimate using responses from
    We also considered competing candidate models at explain-                              diﬀerent viewing conditions. Figure 8A compares the MEG-RDM
ing the MEG-RDMs. First, the Gist model (Oliva and Torralba,                               replicability between the 2D/3D generalized MEG-RDMs with
2001) has previously been successful in explaining early MEG                               the MEG-RDMs during 3D viewing. Interestingly, the 2D/3D
responses for scene stimuli (Henriksson et al., 2019). For our pre-                        generalized MEG-RDMs showed a better replicability in time win-
sent data, the Gist model explained structure in the MEG-RDMs                              dows 70–170 and 190–200 ms compared with the MEG-RDMs
from ∼90 ms from stimulus onset, but the correlation was much                              constructed from the responses during 3D viewing. That is, gener-
lower compared with the other models (Fig. 7A). Next, the mean                             alization across viewing conditions reduced variability in the repre-
distance information in each scene was calculated from the                                 sentational structure at the group level. When compared with the
LiDAR depth data provided with the scene dataset (Adams                                    candidate model RDMs, the 2D/3D generalized MEG-RDMs
et al., 2016). The similarity in scene distance (near–far) correlated                      showed an overall similar result to the results obtained within the
with the MEG-RDMs with a similar time course as the spatial                                viewing conditions (compare Figs. 8B, 6E) with the unique contri-
structure (peak at ∼110 ms), though not providing an equally                               bution of the spatial structure (90–130 ms) being followed by
good ﬁt (Fig. 7B). Mean chromaticity of each scene was also cal-                           semantic content (145–170 ms).
culated, and the Euclidean distance in CIExy color space showed                                Figure 8C compares how well on average the scene pairs could
signiﬁcant correlation with MEG-RDMs in time window                                        be discriminated from the MEG signals when generalized across
110–190 ms (Fig. 7C).                                                                      the 2D and 3D viewing conditions and when evaluated within the
    Finally, we compared the MEG-RDMs with the average RDM                                 3D viewing condition. The results show superiority of the 3D
from the behavioral scene similarity judgments (behavior-RDM).                             viewing condition from ∼150 ms from stimulus onset with on
The behavior-RDM was signiﬁcantly correlated with the                                      average more distinct scene responses during 3D viewing com-
MEG-RDMs from ∼75 ms onward (Fig. 7D), though not provid-                                  pared with the 2D/3D generalized results.
ing an equally good ﬁt in the early time window as the spatial                                 Compared with 2D viewing, the 3D viewing made the scene
structure. Overall, the behavior-RDM had a similar time course                             responses more distinct from ∼150 ms onward (Fig. 8C) but
with the model based on the visual appearance and the semantic                             appeared not to signiﬁcantly aﬀect the emerging representational
content, which was expected based the result that our study                                geometry of our selection of scene stimuli (Figs. 5C, 8).
8 • J. Neurosci., February 26, 2025 • 45(9):e2157232024                                                                                 Mononen et al.   • MEG Responses to 3D Scenes
                                                                                                                                           Taken together, the stereo-
                                                                                                                                       scopic viewing of the scenes
                                                                                                                                       aﬀected the responses relatively
                                                                                                                                       late with onset latency ∼140 ms
                                                                                                                                       (Fig. 9). This timing overlaps
                                                                                                                                       with the time window where
                                                                                                                                       the scenes’ semantic content
                                                                                                                                       explained the MEG-RDMs,
                                                                                                                                       whereas the spatial structure of
                                                                                                                                       the scenes explained the
                                                                                                                                       MEG-RDMs already in an ear-
                                                                                                                                       lier time window of 90–125 ms
                                                                                                                                       (Fig. 6).

                                                                                                                                       Discussion
                                                                                                                                                The human visual system analy-
                                                                                                                                                ses complex real-world visual
                                                                                                                                                scenes rapidly, enabling us to
                                                                                                                                                smoothly navigate and interact
                                                                                                                                                with our visual surroundings.
                                                                                                                                                The aim of the current study
                                                                                                                                                was to characterize how the cor-
                                                                                                                                                tical encoding of a scene’s spatial
                                                                                                                                                structure and semantic content
                                                                                                                                                unfold over time and to examine
                                                                                                                                                the role of 3D disparity cues on
                                                                                                                                                the cortical responses of real-
                                                                                                                                                world scenes. We found that
                                                                                                                                                information related to the spa-
                                                                                                                                                tial structure of the scenes (clut-
                                                                                                                                                tered, closed oﬀ, ﬂat, and
                                                                                                                                                navigable) emerged in the
                                                                                                                                                MEG responses earlier than
                                                                                                                                                information related to the
Figure 6. The contributions of visual appearance, spatial structure, and semantic content in explaining the MEG-RDMs. A, The time course
of the mean rank correlation between the MEG-RDMs and the candidate model based on visual appearance of the scenes is shown. Shaded semantic content (nature, resi-
region indicates the standard-error-of-the-mean (SEM) across participants. Signiﬁcant time points are indicated with red (FDR of 0.05; p values dence,      farm,    car     park).
computed with one-tailed signed rank test across the 20 participants, FDR adjusted across time points). The black bar indicates the stimulus Interestingly, the 3D disparity
on-period (0–500 ms). B, The time course of the mean rank correlation between the MEG-RDMs and the candidate model based on spatial cues from stereoscopic viewing
structure of the scenes is shown. C, The time course of the mean rank correlation between the MEG-RDMs and the candidate model based on of the scenes aﬀected the
semantic content of the scenes is shown. D, Rank correlations with each of the three models are shown in diﬀerent colors (visual appearance in responses relatively late, in a
purple; spatial structure in red; semantic content in blue). The unique contribution of each of these candidate models was evaluated by ﬁtting time window overlapping with
the models to the MEG-RDMs together (black line). The signiﬁcant time points for the combined model are indicated with red (FDR of 0.05; p the processing of the semantic
values computed with one-tailed signed rank test across the 20 participants, FDR adjusted across time points). E, The unique variance content instead of the spatial
explained by the spatial and semantic content are shown, with a zoom-in to the early time window shown below the full time course.
                                                                                                                                                structure of the scene.
Including the spatial structure model signiﬁcantly improved the model ﬁt in time window 90–125 ms. Including the semantic content model
signiﬁcantly improved the model ﬁt in time window 140–175 ms. The visual appearance model did not account for any unique variance.                  The ﬁnding on the rapid
Signiﬁcant time points are indicated with red and blue, corresponding to the two candidate models (FDR of 0.05; p values computed with          encoding     of a scene’s spatial
one-tailed signed rank test across the 20 participants, FDR adjusted across time points).                                                       structure is consistent with pre-
                                                                                                                                                vious M/EEG studies (Cichy et
                                                                                                                                                al., 2017; Lowe et al., 2018;
Difference between scenes viewed in 2D versus 3D                                                      Henriksson et al., 2019; Harel et al., 2022; see, however,
Finally, we directly ask whether we can discriminate the viewing                                      Dwivedi et al., 2024). Lowe et al. (2018) found that the discrimi-
condition from the MEG data. Figure 9A shows the average                                              nation of open versus closed natural scenes was possible from
response discriminability for the same scene between 3D and                                           EEG signals within 100 ms from stimulus onset. Cichy et al.
2D viewing conditions. The viewing condition had a signiﬁcant                                         (2017) measured MEG responses for gray-scale scenes that
eﬀect on the response discriminability from ∼140 ms from sti-                                         diﬀered in physical size and clutter level, and they reported
mulus onset (peak at ∼180 ms). A similar result was obtained                                          that neural representations of clutter level peaked at 107 ms
when comparing the 2D viewing condition with reversed-stereo                                          from stimulus onset whereas representations of scene size
condition (Fig. 9B). The response discriminability between the                                        emerged later. In our recent study, we measured both fMRI
3D and reversed-stereo viewing conditions showed a similar                                            and MEG responses to artiﬁcial natural scenes that had a com-
onset but the average response discriminability was lower com-                                        plete set of all combinations of the ﬁve scene-bounding elements
pared with the comparison with the 2D viewing condition                                               (ﬂoor, ceiling, walls; Henriksson et al., 2019). The fMRI results
(Fig. 9C).                                                                                            showed that the scene-responsive cortical area OPA encodes
Mononen et al.   • MEG Responses to 3D Scenes                                                                                               J. Neurosci., February 26, 2025 • 45(9):e2157232024 • 9




Figure 7. Results from alternative models. The time course of the mean rank correlation between the MEG-RDMs and the alternative candidate models based on (A) low-level features as
captured by the GIST descriptor, (B) scene distance (near–far) as calculated from the LiDAR data provided with the scene images, (C) mean chromaticity of the scenes, and (D) behavioral scene
similarity judgments are shown. Shaded region indicates the standard-error-of-the-mean (SEM) across participants. Signiﬁcant time points are indicated with red (FDR of 0.05; p values computed
with one-tailed signed rank test across the 20 participants, FDR adjusted across time points). The black bar indicates the stimulus on-period (0–500 ms). E, Pairwise rank correlations of all
model-RDMs are shown.


                                                                                                    scene-layout with invariance to changes in surface texture (see
                                                                                                    also Lescroart and Gallant, 2019), and complementary MEG evi-
                                                                                                    dence indicated that this texture-invariant scene-layout repre-
                                                                                                    sentation emerges within ∼100 ms (Henriksson et al., 2019).
                                                                                                    Harel et al. (2022) studied cortical encoding of navigational aﬀor-
                                                                                                    dances by measuring EEG responses while participants viewed
                                                                                                    pictures of computer-generated room scenes that had diﬀerent
                                                                                                    numbers of doors. They found that the amplitude of the P2
                                                                                                    ERP was modulated by the navigability of the scenes and that
                                                                                                    the number of doors and their locations could be decoded
                                                                                                    from the EEG responses at even earlier time windows.
                                                                                                    Altogether, the rapid cortical encoding of a scene’s spatial struc-
                                                                                                    ture and navigational aﬀordances supports smooth navigation
                                                                                                    through the immediately visible environment.
                                                                                                        Our study participants only used semantic content and visual
                                                                                                    appearance when making behavioral judgments of the scene sim-
                                                                                                    ilarity, but cortical responses were evidently aﬀected by the spa-
                                                                                                    tial structure of the scenes. Similarly, Groen et al. (2018) used a
                                                                                                    multi-arrangement task similar to ours with scene stimuli and
                                                                                                    found a dissociation between behavioral and fMRI results.
                                                                                                    Their behavior-RDM was best captured by a functional model
                                                                                                    of potential actions in a scene whereas their fMRI-RDMs from
                                                                                                    scene-responsive areas were better captured by mid-level layers
                                                                                                    of a deep neural network (DNN) model. Though our behavioral
                                                                                                    result could reﬂect participants’ implicit tendency to interpret the
                                                                                                    similarity judgment task as a categorization task and therefore to
                                                                                                    arrange the scenes in terms of their basic level category member-
                                                                                                    ship (e.g., beach, mountain; Tversky and Hemenway, 1983), the
                                                                                                    dissociation of the behavioral and early brain responses could
                                                                                                    also reﬂect a more profound diﬀerence between the spatial and
                                                                                                    semantic dimensions. Previously, Greene and Oliva (2009a,b)


                                                                                                    
                                                                                                    candidate models (Fig. 3) were ﬁtted together to the 2D/3D generalized MEG-RDMs (FDR of
Figure 8. Generalization across viewing conditions. A, The replicability of the MEG-RDMs            0.05; p values computed with one-tailed signed rank test across the 20 participants, FDR
during 3D viewing (shown in black) are compared with the replicability of the MEG-RDMs              adjusted across time points; compare with Fig. 5E). C, The average scene discriminability dur-
generalized across 2D and 3D viewing conditions (shown in red). Time points with signiﬁcant         ing 3D viewing (shown in black) is compared with the average scene discriminability gener-
diﬀerence are indicated with red (FDR of 0.05; p values computed with two-tailed signed rank        alized between 2D and 3D viewing (shown in red). Time points with signiﬁcant diﬀerence are
test across the 20 participants, FDR adjusted across time points). B, Spatial structure (shown in   indicated with red (FDR of 0.05; p values computed with two-tailed signed rank test across
red) followed by semantic content (shown in blue) explained unique variance when the three          the 20 participants, FDR adjusted across time points).
10 • J. Neurosci., February 26, 2025 • 45(9):e2157232024                                                                                         Mononen et al.   • MEG Responses to 3D Scenes




Figure 9. Scenes viewed in 2D versus 3D. A, The average discriminability between the same scene viewed in 2D and stereoscopically in 3D is shown, with a zoom-in to the early time window
shown below the full time course. Shaded region indicates the standard-error-of-the-mean (SEM) across participants. Signiﬁcant time points are indicated with red (FDR of 0.05; p values
computed with one-tailed signed rank test across the 20 participants, FDR adjusted across time points). The black bar indicates the stimulus on-period (0–500 ms). B, Average discriminability
between a scene viewed in 2D and in reversed-stereo viewing condition. C, Average discriminability between a scene viewed in 3D and in reversed-stereo viewing condition.


showed that human observers perceive spatial dimensions, such                                    automatically encodes spatial structure and navigability of our local
as the mean depth or openness of a scene, from a shorter presen-                                 visual surroundings. An open question remains how this informa-
tation time compared with the basic level category and concluded                                 tion is transformed and used while acting in the local visual envi-
that spatial aspects precede category information in scene recog-                                ronment in comparison with passive viewing.
nition. Furthermore, they argued that the entry level for commu-                                     Few previous M/EEG studies have studied how binocular depth
nication about scenes may still be the basic level category.                                     cues in natural scenes aﬀect cortical responses (Fischmeister and
Together, these ﬁndings suggest that cortical processing of a                                    Bauer, 2006, Duan et al., 2018, 2021). Fischmeister and Bauer
scene’s spatial structure occurs rapidly and likely automatically.                               (2006) presented diﬀerent scenes in stereoscopic, 2D and scram-
    The three cortical scene-responsive areas OPA, PPA, and retro-                               bled modes during EEG recordings with the aim to localize the
splenial complex (RSC) have been suggested to be dissociated based                               cortical areas activated when switching, for example, from 2D to
on whether their processing is automatic or deliberate (for a review,                            stereoscopic viewing. Their main conclusions were on the feasibil-
see Dilks et al., 2022). By comparing passive and active navigation                              ity of the use of stereoscopic natural images as stimuli in EEG
tasks in simple and complex environments, a recent fMRI study                                    experiments with no deﬁnite conclusions on the location or timing
showed the RSC to be involved in deliberate navigation through                                   of the stereoscopic depth cues. More recently, Duan et al. (2018,
the broader spatial environment, whereas the scene-responsive cor-                               2021) applied a method called reliable-component analysis to
tical area OPA was shown to support navigation through the local                                 EEG data obtained during viewing of stereo-image pairs and 2D
visual environment automatically (Suzuki et al., 2021). The central                              images of outdoor scenes. They reported a diﬀerence between
role of OPA in the automatic extraction of navigational aﬀordances                               the 2D and 3D scenes starting as early as 95 ms from stimulus
of the immediate environment was also demonstrated in the fMRI                                   onset and that the diﬀerential response between the 2D and 3D
study by Bonner and Epstein (2017) and was later attributed to                                   scenes was aﬀected by the high-level scene structure (Duan
purely feedforward computations (Bonner and Epstein, 2018).                                      et al., 2018). In a subsequent study, Duan et al. (2021) indepen-
Interestingly, a recent study where participants were explicitly                                 dently varied the monocular scene content and the stereoscopic
asked to identify navigational paths in scene images during EEG                                  cues in natural scene stimuli. They reported an interaction with
data collection suggested that this information is processed signiﬁ-                             the monocular scene content and the stereoscopic depth cues start-
cantly later compared with more basic scene features, such as edges                              ing from 150 ms with scene-speciﬁc and perceptually predictive
or depth, or the scene’s semantic content (Dwivedi et al., 2024). In                             responses (Duan et al., 2021). This timing is consistent with
the results presented here, the timing of the eﬀect of scenes’ spatial                           what we found, namely, that stereoscopic viewing of the scenes
structure on the MEG responses together with the negligible eﬀect                                aﬀected the responses with the onset latency ∼140 ms.
of the spatial structure on the behavioral results are consistent with                               In our results, the encoding of the scene’s spatial structure
the view that our visual system, most likely OPA, rapidly and                                    precedes the encoding of the semantic content and the sensitivity
Mononen et al.   • MEG Responses to 3D Scenes                                                                       J. Neurosci., February 26, 2025 • 45(9):e2157232024 • 11


to stereoscopic depth cues. The temporal overlap between the                       Brainard DH (1997) The psychophysics toolbox. Spat Vis 10:433–436.
semantic content and stereoscopic depth cues suggests that the                     Cichy RM, Khosla A, Pantazis D, Oliva A (2017) Dynamics of scene represen-
                                                                                       tations in the human brain revealed by magnetoencephalography and
stereoscopic depth cues may not be critical for encoding of a
                                                                                       deep neural networks. Neuroimage 153:346–358.
scene’s spatial structure but rather beneﬁt object segmentation.                   Cutting JE, Vishton PM (1995) Perceiving layout and knowing distances: the
Moreover, the similarity in the timing of the decoding results                         integration, relative potency, and contextual use of different information
between the 2D versus 3D and 2D versus reversed-stereo viewing                         about depth. In: Perception of space and motion (Epstein W, Rogers S,
condition of the same scene further suggests that the disparity                        eds), pp 69–117. San Diego: Academic Press.
cues are related to ﬁgure-and-ground separation. This interpre-                    Dilks DD, Julian JB, Paunov AM, Kanwisher N (2013) The occipital place area
tation is consistent with previous studies on the advantage of                         is causally and selectively involved in scene perception. J Neurosci 33:
                                                                                       1331–1336.
stereoscopic depth cues on object recognition (Adams et al.,                       Dilks DD, Kamps FS, Persichetti AS (2022) Three cortical scene systems and
2019; Anderson et al., 2022).                                                          their development. Trends Cogn Sci 26:117–127.
    Ultimately, we are interested in how the human visual system                   Duan Y, Thatte J, Yaklovleva A, Norcia AM (2021) Disparity in context:
understands and enables the smooth interaction with our natural                        understanding how monocular image content interacts with disparity
3D environments. In the present study, we used a relatively small                      processing in human visual cortex. Neuroimage 237:118139.
number of picture stimuli typical to brain imaging experiments.                    Duan Y, Yakovleva A, Norcia AM (2018) Determinants of neural responses to
                                                                                       disparity in natural scenes. J Vis 18:21.
The stereoscopic depth cues brought vividness to the scene stimuli                 Dwivedi K, Sadiya S, Balode MP, Roig G, Cichy RM (2024) Visual features are
but had nonetheless surprisingly little eﬀect on the cortical                          processed before navigational affordances in the human brain. Sci Rep 14:
responses. A possible limitation here is the use of outdoor natural                    5573.
images, taken from a distance, as the stimuli. Binocular cues contrib-             Epstein RA, Baker CI (2019) Scene perception in the human brain. Annu Rev
ute to depth judgments at far distances (Palmisano et al., 2010;                       Vis Sci 5:373–397.
McCann et al., 2018), but they are most eﬀective in the space within               Epstein R, Kanwisher N (1998) A cortical representation of the local visual
                                                                                       environment. Nature 392:598–601.
“grasping distance” (Cutting and Vishton, 1995). Hence, future
                                                                                   Fischmeister FPS, Bauer H (2006) Neural correlates of monocular and bino-
studies could complement the present ﬁndings with close-up views                       cular depth cues based on natural images: a LORETA analysis. Vision Res
of scenes. Such reachable-scale views of environments were recently                    46:3373–3380.
suggested to have distinct cortical representations from objects and               Gramfort A, Luessi M, Larson E, Engemann DA, Strohmeier D, Brodbeck C,
navigable environments (Josephs and Konkle, 2020) and could also                       Goj R, Jas M, Brooks T, Parkkonen L (2013) MEG and EEG data analysis
show distinct response proﬁle to diﬀerent depth cues. Furthermore,                     with MNE-python. Front Neurosci 267:1–13.
an exciting direction for future research is to mimic real-world 3D                Greene MR, Hansen BC (2020) Disentangling the independent contributions
                                                                                       of visual and conceptual features to the spatiotemporal dynamics of scene
visual environments and realistic visual tasks using virtual reality                   categorization. J Neurosci 40:5283–5299.
during brain imaging. Virtual environments would allow controlled                  Greene MR, Oliva A (2009a) The briefest of glances: the time course of natural
manipulation of diﬀerent depth cues and spatial layouts. Such sti-                     scene understanding. Psychol Sci 20:464–472.
mulus sets would likely facilitate to more directly tease apart the                Greene MR, Oliva A (2009b) Recognition of natural scenes from global prop-
roles of diﬀerent monocular and binocular cues for object segmen-                      erties: seeing the forest without representing the trees. Cogn Psychol 58:
tation and overall layout perception compared with the relatively                      137–176.
                                                                                   Grill-Spector K (2003) The neural basis of object perception. Curr Opin
small number of real-world scenes that were tested in the present                      Neurobiol 13:159–166.
study. Challenges with naturalistic virtual reality experiments lie,               Groen II, Greene MR, Baldassano C, Fei-Fei L, Beck DM, Baker CI (2018)
however, both in the implementation of the experimental setups                         Distinct contributions of functional and deep neural network features
and in the analysis and interpretation of the complex data.                            to representational similarity of scenes in human brain and behavior.
                                                                                       Elife 7:e32962.
                                                                                   Groen IIA, Silson EH, Baker CI (2017) Contributions of low- and high-level
References                                                                             properties to neural processing of visual scenes in the human brain. Philos
Adams WJ, Elder JH, Graf EW, Leyland J, Lugtigheid AJ, Muryy A (2016) The              Trans R Soc Lond B Biol Sci 372:20160102.
    Southampton-York Natural Scenes (SYNS) dataset: statistics of surface          Guggenmos M, Sterzer P, Cichy RM (2018) Multivariate pattern analysis
    attitude. Sci Rep 6:35805.                                                         for MEG: a comparison of dissimilarity measures. Neuroimage 173:
Adams WJ, Graf EW, Anderson M (2019) Disruptive coloration and binocu-                 434–447.
    lar disparity: breaking camouﬂage. Proc Biol Sci 286:20182045.                 Harel A, Nador JD, Bonner MF, Epstein RA (2022) Early electrophysiological
Anderson MD, Elder JH, Graf EW, Adams WJ (2022) The time-course of                     markers of navigational affordances in scenes. J Cogn Neurosci 34:397–
    real-world scene perception: spatial and semantic processing. Iscience             410.
    25:105633.                                                                     Henriksson L, Mur M, Kriegeskorte N (2019) Rapid invariant encoding of
Anderson M, Graf E, Elder JH, Ehinger K, Adams W (2020) Dataset for: cat-              scene layout in human OPA. Neuron 103:161–171.
    egory systems for real-world scenes. Available at: https://eprints.soton.ac.   Hyvärinen A, Oja E (2000) Independent component analysis: algorithms and
    uk/446699/                                                                         applications. Neural Netw 13:411–430.
Anderson MD, Graf EW, Elder JH, Ehinger KA, Adams WJ (2021) Category               Josephs EL, Konkle T (2020) Large-scale dissociations between views of
    systems for real-world scenes. J Vis 21:8.                                         objects, scenes, and reachable-scale environments in visual cortex. Proc
Arbuckle SA, Yokoi A, Pruszynski JA, Diedrichsen J (2019) Stability of repre-          Natl Acad Sci U S A 117:29354–29362.
    sentational geometry across a wide range of fMRI activity levels.              Khaligh-Razavi S-M, Kriegeskorte N (2014) Deep supervised, but not unsu-
    Neuroimage 186:155–163.                                                            pervised, models may explain IT cortical representation. PLoS Comput
Baillet S (2017) Magnetoencephalography for brain electrophysiology and                Biol 10:e1003915.
    imaging. Nat Neurosci 20:327–339.                                              Kleiner M, Brainard D, Pelli D (2007) What’s new in psychtoolbox-3?
Bartnik CG, Groen II (2023) Visual perception in the human brain: how the              Perception 36:1.
    brain perceives and understands real-world scenes. In: Oxford research         Kriegeskorte N, Kievit RA (2013) Representational geometry: integrating
    encyclopedia of neuroscience. Oxford, UK: Oxford University Press.                 cognition, computation, and the brain. Trends Cogn Sci 17:401–412.
Bonner MF, Epstein RA (2017) Coding of navigational affordances in the             Kriegeskorte N, Mur M (2012) Inverse MDS: inferring dissimilarity structure
    human visual system. Proc Natl Acad Sci U S A 114:4793–4798.                       from multiple item arrangements. Front Psychol 3:245.
Bonner MF, Epstein RA (2018) Computational mechanisms underlying cor-              Kriegeskorte N, Mur M, Bandettini PA (2008) Representational similarity
    tical responses to the affordance properties of visual scenes. PLoS Comput         analysis-connecting the branches of systems neuroscience. Front Syst
    Biol 14:e1006111.                                                                  Neurosci 2:4.
12 • J. Neurosci., February 26, 2025 • 45(9):e2157232024                                                               Mononen et al.   • MEG Responses to 3D Scenes
Kurki I, Hyvärinen A, Henriksson L (2022) Dynamics of retinotopic spatial       Piano ME, Tidbury LP, O’Connor AR (2016) Normative values for near and
    attention revealed by multifocal MEG. Neuroimage 263:119643.                   distance clinical tests of stereoacuity. Strabismus 24:169–172.
Lescroart MD, Gallant JL (2019) Human scene-selective areas represent 3D        Ramkumar P, Hansen BC, Pannasch S, Loschky LC (2016) Visual informa-
    conﬁgurations of surfaces. Neuron 101:178–192.                                 tion representation and rapid-scene categorization are simultaneous
Lescroart MD, Stansbury DE, Gallant JL (2015) Fourier power, subjective            across cortex: an MEG study. Neuroimage 134:295–304.
    distance, and object categories all provide plausible models of BOLD        Suzuki S, Kamps FS, Dilks DD, Treadway MT (2021) Two scene navigation
    responses in scene-selective visual areas. Front Comput Neurosci 9:135.        systems dissociated by deliberate versus automatic processing. Cortex
Lowe MX, Rajsic J, Ferber S, Walther DB (2018) Discriminating scene                140:199–209.
    categories from brain activity within 100 milliseconds. Cortex 106:275–     Tarhan L, De Freitas J, Konkle T (2021) Behavioral and neural representations
    287.                                                                           en route to intuitive action understanding. Neuropsychologia 163:108048.
Malcolm GL, Groen II, Baker CI (2016) Making sense of real-world scenes.        Taulu S, Simola J (2006) Spatiotemporal signal space separation method for
    Trends Cogn Sci 20:843–856.                                                    rejecting nearby interference in MEG measurements. Phys Med Biol 51:
McCann BC, Hayhoe MM, Geisler WS (2018) Contributions of monocular                 1759–1768.
    and binocular cues to distance discrimination in natural scenes. J Vis      Tversky B, Hemenway K (1983) Categories of environmental scenes. Cogn
    18:12.                                                                         Psychol 15:121–149.
Nili H, Wingﬁeld C, Walther A, Su L, Marslen-Wilson W, Kriegeskorte N           Vishwanath D (2023) From pictures to reality: modelling the phenomenology
    (2014) A toolbox for representational similarity analysis. PLoS Comput         and psychophysics of 3D perception. Philos Trans R Soc Lond B Biol Sci
    Biol 10:e1003553.                                                              378:20210454.
Oliva A, Torralba A (2001) Modeling the shape of the scene: a holistic repre-   Walther A, Nili H, Ejaz N, Alink A, Kriegeskorte N, Diedrichsen J (2016)
    sentation of the spatial envelope. Int J Comput Vis 42:145–175.                Reliability of dissimilarity measures for multi-voxel pattern analysis.
Palmisano S, Gillam B, Govan DG, Allison RS, Harris JM (2010) Stereoscopic         Neuroimage 137:188–200.
    perception of real depths at large distances. J Vis 10:19.                  Wheatstone C (1838) XVIII. Contributions to the physiology of vision.—Part
Pelli DG (1997) The VideoToolbox software for visual psychophysics: trans-         the ﬁrst. On some remarkable, and hitherto unobserved, phenomena of
    forming numbers into movies. Spat Vis 10:437–442.                              binocular vision. Philos Trans R Soc Lond B Biol Sci 128:371–394.
